| 日期 | 標題 | 摘要 | 主題 |
|------|-------|----------|--------|
| 2024年6月28日 | [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/abs/2406.18629) | 數學推理對大型語言模型（LLM）構成了重大挑戰，因為它需要精確且長鏈的推理來確保準確性。確保每個推理步驟的正確性至關重要。為了解決這個問題，我們旨在通過學習來自人類反饋的方法來增強LLM的魯棒性和真實性。然而，直接偏好優化（DPO）對於長鏈數學推理顯示出有限的好處，因為使用DPO的模型難以識別錯誤答案中的詳細錯誤。這種限制源於缺乏細粒度的過程監督。我們提出了一種簡單、有效且數據高效的方法，稱為Step-DPO，將個別推理步驟視為偏好優化的單位，而不是整體評估答案。此外，我們還開發了一個數據構建管道，用於Step-DPO，從而創建包含1萬個步驟偏好對的高質量數據集。我們還觀察到，在DPO中，自生成數據比由人類或GPT-4生成的數據更有效，因為後者具有分佈外的性質。我們的研究結果表明，僅需1萬個偏好數據對和少於500個Step-DPO訓練步驟，即可使擁有超過70B參數的模型在數學測試中準確性提高近3%。值得注意的是，將Step-DPO應用於Qwen2-72B-Instruct時，在MATH和GSM8K的測試集中分別達到了70.8%和94.0%的成績，超越了一系列閉源模型，包括GPT-4-1106、Claude-3-Opus和Gemini-1.5-Pro。 | 數學推理、優化 |
| 2024年6月28日 | [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094) | 我們提出了一種新穎的基於角色驅動的數據合成方法，利用大型語言模型（LLM）中的多種視角來創建多樣的合成數據。為了在規模上充分利用這種方法，我們引入了Persona Hub，這是一個從網絡數據中自動策劃的包含10億個多樣化角色的集合。這10億個角色（約佔世界總人口的13%）作為分佈式知識載體，幾乎可以觸及LLM中包含的每一個視角，從而促進在各種場景下的大規模多樣化合成數據的創建。通過展示Persona Hub在合成高質量數學和邏輯推理問題、指令（即用戶提示）、知識豐富的文本、遊戲NPC和工具（函數）方面的應用，我們展示了基於角色的數據合成方法具有多用途、可擴展、靈活和易於使用的特點，可能推動合成數據創建和應用的範式轉變，並對LLM研究和開發產生深遠影響。 | 合成數據生成 |
| 2024年6月27日 | [WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510) | 我們介紹了WildTeaming，一個自動化LLM安全紅隊框架，從野外用戶與聊天機器人的互動中發現5.7K個新型越獄策略，然後組合多種策略以系統性地探索新型越獄。與以往通過招募人類工作者、基於梯度的優化或LLM的迭代修訂進行紅隊操作的工作相比，我們的工作研究了未專門指導用戶進行系統越獄的情況。WildTeaming揭示了前沿LLM的未發現漏洞，比現有的越獄方法達到最多4.6倍的多樣性和成功率。雖然有許多數據集存在於越獄評估中，但很少有開源數據集存在於越獄訓練中，因為安全訓練數據在模型權重開放時仍然是封閉的。通過WildTeaming，我們創建了WildJailbreak，一個大規模的開源合成安全數據集，包含262K個普通（直接請求）和對抗性（複雜越獄）提示-回應對。為了減輕誇張的安全行為，WildJailbreak提供了兩種對比類型的查詢：1）有害查詢（普通和對抗性）和2）形式上類似於有害查詢但不含任何危害的良性查詢。由於WildJailbreak顯著提升了現有安全資源的質量和規模，它獨特地使我們能夠檢驗數據規模效應以及數據屬性和模型能力在安全訓練中的相互作用。通過廣泛的實驗，我們識別出能夠實現安全行為理想平衡的訓練屬性：適當的保護而不過度拒絕，有效處理普通和對抗性查詢，並且對一般能力的下降極少。WildJailbreak的所有組件都為實現模型的平衡安全行為做出了貢獻。 | 紅隊操作、LLM攻擊 |
| 2024年6月27日 | [LiveBench: A Challenging, Contamination-Free LLM Benchmark](https://arxiv.org/abs/2406.19314) | 測試集污染（測試數據進入新模型的訓練集中）是公平LLM評估的已知障礙，並會迅速使基準測試失效。為了減輕這一問題，許多最新的基準測試通過人類或LLM評審進行新提示和評估的眾包；然而，這些可能引入顯著的偏見，並在評分困難問題時失效。在這項工作中，我們引入了一個新的LLM基準測試，旨在避免測試集污染和LLM評審與人類眾包的陷阱。我們發布了LiveBench，這是第一個（1）包含來自最新信息來源的經常更新的問題，（2）根據客觀真實值自動評分的基準測試，（3）包含各種具有挑戰性的任務，涵蓋數學、編程、推理、語言、指令跟隨和數據分析。為了實現這一目標，LiveBench包含基於最近發布的數學競賽、arXiv論文、新聞文章和數據集的問題，並包含來自先前基準測試（如Big-Bench Hard、AMPS和IFEval）的更難的無污染版本。我們評估了許多著名的閉源模型以及從0.5B到110B大小的數十個開源模型。LiveBench很難，頂級模型的準確率不到65%。我們發布所有問題、代碼和模型答案。問題將每月添加和更新，我們將隨著時間的推移發布新任務和更難的任務版本，以便LiveBench能夠區分LLM隨著能力的提高而改進的能力。我們歡迎社區參與和合作，以擴展基準任務和模型。 | 基準測試、數據集 |
| 2024年6月26日 | [Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2406.18676) | 檢索增強生成（RAG）在減少大型語言模型（LLM）幻覺問題方面顯示出有效性。然而，使檢索器與多樣化的LLM知識偏好對齊的困難不可避免地對開發可靠的RAG系統構成了挑戰。為了解決這一問題，我們提出了DPA-RAG，一個旨在對齊RAG系統內多樣化知識偏好的通用框架。具體來說，我們最初引入了一個偏好知識構建管道，並採用五種新穎的查詢增強策略來緩解偏好數據稀缺問題。基於偏好數據，DPA-RAG實現了內部和外部偏好對齊：1）它將對排序器整合配對、點對和對比偏好對齊能力，實現RAG組件之間的外部偏好對齊。2）它在常規監督微調（SFT）之前引入了一個預對齊階段，使LLM能夠隱式捕獲與其推理偏好對齊的知識，實現LLM的內部對齊。跨越四個知識密集型問答數據集的實驗結果表明，DPA-RAG超越了所有基線，並無縫集成了黑盒和開源的LLM閱讀器。進一步的定性分析和討論也為實現可靠的RAG系統提供了經驗指導。 | RAG、對齊 |
| 2024年6月21日 | [LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs](https://arxiv.org/abs/2406.15319) | 在傳統的RAG框架中，基本檢索單位通常很短。常見的檢索器如DPR通常使用100字的維基百科段落。這樣的設計迫使檢索器在大規模語料庫中搜尋找到「針」的單位。相比之下，閱讀器只需從短檢索單位中提取答案。這種不平衡的「重」檢索器和「輕」閱讀器設計可能導致次優的性能。為了解決這種不平衡，我們提出了一個新框架LongRAG，包括一個「長檢索器」和一個「長閱讀器」。LongRAG將整個維基百科處理成4K字符單位，比之前長30倍。通過增加單位大小，我們顯著減少了總單位數量，從2200萬減少到70萬。這顯著降低了檢索器的負擔，導致了顯著的檢索得分：NQ的答案召回率@1=71%（之前為52%），HotpotQA（全維基）的答案召回率@2=72%（之前為47%）。然後我們將前k個檢索單位（約3萬字符）輸入現有的長上下文LLM進行零樣本答案提取。在不需要任何訓練的情況下，LongRAG在NQ上達到了62.7%的EM，這是已知的最佳結果。LongRAG在HotpotQA（全維基）上也達到了64.3%，與SoTA模型相當。我們的研究提供了將RAG與長上下文LLM結合的未來路線圖的見解。 | RAG |
| 2024年6月20日 | [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | 今天，我們推出了Claude 3.5 Sonnet——我們即將推出的Claude 3.5模型系列中的首個發布。Claude 3.5 Sonnet在多種評估中表現優於競爭對手模型和Claude 3 Opus，具有中檔模型Claude 3 Sonnet的速度和成本。 | 基礎LLM |
| 2024年6月20日 | [Can LLMs Learn by Teaching? A Preliminary Study](https://arxiv.org/abs/2406.14629) | 教學以提高學生模型（如知識蒸餾）是LLM中廣泛研究的方法。然而，對於人類來說，教學不僅提高了學生，也提高了教師。我們問：LLM是否也能通過教學學習（LbT）？如果是，我們可以潛在地解鎖不僅依賴於人類生成數據或更強模型的持續進步可能性。在本文中，我們對這一雄心勃勃的議程進行了初步探索。我們展示了可以將LbT理念融入現有的LLM訓練/提示管道，並提供顯著改進。具體來說，我們設計了三種方法，每種模仿人類LbT的三個層次之一：觀察學生的反饋、從反饋中學習和迭代學習，目的是在不進行訓練的情況下提高答案準確性，並通過微調提高模型的內在能力。結果令人鼓舞。例如，類似於人類的LbT，我們看到：（1）LbT可以引發弱到強的推廣：強模型可以通過教弱模型來改善自己；（2）學生的多樣性可能有幫助：教多個學生可能比教一個學生或教師本身更好。我們希望這種早期的承諾能激發對LbT的未來研究，以及更廣泛地採用先進的教育技術來改進LLM。代碼可在https://github.com/imagination-research/lbt獲得。 | LLM學習 |
| 2024年6月19日 | [Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?](https://arxiv.org/abs/2406.13121) | 長上下文語言模型（LCLM）有潛力革新我們處理傳統上依賴於外部工具（如檢索系統或數據庫）任務的方法。利用LCLM本身攝取和處理整個語料庫的信息提供了許多優勢。它通過消除對工具的專業知識需求來提高用戶友好性，提供堅固的端到端建模，從而最大限度地減少複雜管道中的連鎖錯誤，並允許在整個系統中應用複雜的提示技術。為了評估這一範式轉變，我們引入了LOFT，一個需要上下文達到數百萬個字符的真實任務基準，用於評估LCLM在上下文檢索和推理中的表現。我們的研究結果顯示，LCLM出人意料地能夠競爭最先進的檢索和RAG系統，儘管從未明確訓練過這些任務。然而，LCLM仍在需要SQL類任務的組合推理方面面臨挑戰。值得注意的是，提示策略顯著影響性能，強調隨著上下文長度增加需要持續研究。總體而言，LOFT提供了一個嚴格的測試平台，展示了LCLM取代現有範式並隨著模型能力的擴展解決新任務的潛力。 | 長上下文、分析 |
| 2024年6月18日 | [Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](https://arxiv.org/abs/2406.12624) | 提供一個有前途的解決方案來應對與人類評估相關的可擴展性挑戰，LLM作為評審範式正在迅速獲得關注。然而，關於這一範式的優勢和劣勢以及其潛在偏見仍有許多未解問題。在本文中，我們對各種LLM作為評審的表現進行了全面研究。我們利用TriviaQA作為評估LLM客觀知識推理的基準，並與人類註釋一起進行評估，發現它們具有高度一致性。我們的研究包括9個評審模型和9個考試模型——包括基礎模型和指令調優模型。我們評估了評審模型在不同模型大小、系列和評審提示上的對齊情況。研究結果之一顯示，使用Cohen's kappa作為對齊度量標準比簡單的百分比一致性更為重要，因為具有高百分比一致性的評審仍可能給出完全不同的分數。我們發現Llama-3 70B和GPT-4 Turbo在人類對齊方面表現出色，但在考試模型排名方面，JudgeLM-7B和詞法評審Contains表現更好，後者在人類對齊度低34點。通過錯誤分析和其他研究，包括指令長度和寬容偏見的影響，我們希望為未來使用LLM作為評審提供有價值的經驗教訓。 | 評估 |
| 2024年6月18日 | [From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries](https://arxiv.org/abs/2406.12824) | 檢索增強生成（RAG）豐富了語言模型利用外部上下文來增強對給定用戶提示的響應能力。這種方法因其在搜尋、問答和聊天機器人的各種應用中顯示出的實用性而流行。然而，這種方法的具體運作方式尚不明確。在本文中，我們機械地檢查了RAG管道，強調語言模型在回答問題時傾向於只利用上下文信息，而最小限度依賴其參數記憶。我們通過因果中介分析展示了語言模型在回答問題時最小化利用參數記憶，並通過注意力貢獻和淘汰顯示最後一個字符殘差流未從問題中的主題字符獲得豐富，而是從上下文中的其他信息字符中獲得豐富。我們發現這種顯著的捷徑行為在LLaMa和Phi系列模型中都存在。 | RAG、知識整合 |
| 2024年6月18日 | [PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers](https://arxiv.org/abs/2406.12430) | 由於沒有基準可以檢驗決策QA，我們提出了決策QA基準（Decision QA benchmark，DQA）。它有兩個場景：定位和建造，分別來自兩個視頻遊戲（《歐洲帝國IV》和《維多利亞3》），它們與Decision QA目標相同。為了有效應對決策QA，我們還提出了一種新的RAG技術，稱為迭代計劃-檢索增強生成（PlanRAG）。我們的基於PlanRAG的LM首先生成決策計劃作為第一步，檢索器生成數據分析的查詢作為第二步。該方法在定位場景中比最先進的迭代RAG方法高出15.8%，在建造場景中高出7.4%。我們在這個https URL上發布了我們的代碼和基準。 | RAG、知識整合 |
| 2024年6月17日 | [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://arxiv.org/abs/2406.12034) | 我們提出了Self-MoE，一種將單體LLM轉變為由自專家組成的組合模塊化系統的方法，稱為MiXSE（自專家混合體）。我們的方法利用自專家化，構建了使用自生成合成數據的專家模塊，每個模塊配備了一個共享基礎LLM，並結合自優化路由。這允許動態和特定能力的處理各種目標任務，增強整體能力，而不需要廣泛的人工標記數據和額外的參數。我們的實驗結果顯示，專業化的LLM在非專業化任務上可能表現出潛在的權衡。然而，我們的Self-MoE在知識、推理、數學和編碼等多個基準上顯示出顯著的改進。它還一貫超越其他方法，包括實例合併和權重合併，同時在設計上提供更好的靈活性和可解釋性，具有語義專家和路由。我們的研究結果突出了模塊化的重要性以及自我改進在實現高效、可擴展和適應性系統中的潛力。 | 自專家混合體、LLM架構 |
| 2024年6月17日 | [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](https://arxiv.org/abs/2406.11839) | 直接偏好優化（DPO）已被證明是對齊大型語言模型（LLM）的有效方法。最近的研究嘗試將DPO應用於多模態場景，但發現很難實現一致的改進。通過比較實驗，我們確定了多模態偏好優化中的無條件偏好問題，即模型忽視圖像條件。為了解決這一問題，我們提出了mDPO，一種多模態DPO目標，防止過度優先考慮僅語言偏好，還優化圖像偏好。此外，我們引入了一個獎勵錨點，強制選擇的回應的獎勵為正，從而避免其可能性的降低——這是相對偏好優化的內在問題。針對兩個不同規模的多模態LLM和三個廣泛使用的基準的實驗表明，mDPO有效解決了多模態偏好優化中的無條件偏好問題，並顯著提高了模型性能，特別是在減少幻覺方面。 | 優化 |
| 2024年6月15日 | [SELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326) | 大型語言模型（LLM）經常因為一次性訓練和世界不斷變化的性質而難以提供最新的資訊。為了使LLM保持最新，現有的方法通常涉及在新文件上持續預訓練。然而，它們經常面臨提取儲存知識的困難。受費曼技巧在人類高效學習中的顯著成功的啟發，我們引入了SELF-TUNING，一個旨在提高LLM從原始文件中有效獲取新知識的學習框架，通過自我教學。具體而言，我們開發了一個SELFTEACHING策略，通過自監督方式創建的一組知識密集型任務增強文件，專注於記憶、理解和自我反思三個方面。此外，我們引入了三個Wiki-Newpages-2023-QA數據集，以促進LLM在記憶、提取和推理方面的知識獲取能力的深入分析。跨越LLAMA2系列模型的廣泛實驗結果顯示，SELF-TUNING在所有知識獲取任務中始終表現優異，並在保留先前知識方面表現出色。 | LLM訓練、知識整合 |
| 2024年6月15日 | [DeepSeek-Coder-V2](https://github.com/deepseek-ai/DeepSeek-Coder-V2) | 我們推出了DeepSeek-Coder-V2，一個開源的專家混合體（MoE）代碼語言模型，在代碼特定任務上達到與GPT4-Turbo相當的性能。具體來說，DeepSeek-Coder-V2從DeepSeek-V2的中間檢查點進一步預訓練，增加了6萬億個字符。通過這種持續的預訓練，DeepSeek-Coder-V2大幅增強了DeepSeek-V2的編碼和數學推理能力，同時在一般語言任務中保持相當的性能。與DeepSeek-Coder-33B相比，DeepSeek-Coder-V2在各方面的代碼相關任務中顯示出顯著的進步，以及推理和一般能力的提升。此外，DeepSeek-Coder-V2擴展了對程式語言的支持，從86種擴展到338種，並將上下文長度從16K擴展到128K。 | 特定領域LLM |
| 2024年6月14日 | [Nemotron-4 340B Technical Report](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf) | 我們發布了Nemotron-4 340B模型系列，包括Nemotron-4-340B-Base、Nemotron-4-340B-Instruct和Nemotron-4-340B-Reward。我們的模型在NVIDIA開放模型許可協議下開放訪問，這是一個允許分發、修改和使用模型及其輸出的寬鬆模型許可。這些模型在廣泛的評估基準上與開放訪問模型競爭，並且在FP8精度部署時適合在單個DGX H100上運行。我們相信社區可以在各種研究和商業應用中受益於這些模型，特別是用於生成合成數據以訓練較小的語言模型。值得注意的是，我們模型對齊過程中使用的數據超過98%是合成生成的，展示了這些模型在生成合成數據方面的有效性。為了進一步支持開放研究並促進模型開發，我們發布了 | 基礎LLM |
| 2024年6月14日 | [Open-Sora 1.2](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_03.md) | 我們設計並實施了Open-Sora，一個致力於高效生成高質量視頻的計劃。我們希望使模型、工具和所有細節對所有人都可以訪問。通過擁抱開源原則，Open-Sora不僅使先進的視頻生成技術民主化，還提供了一個精簡且用戶友好的平台，簡化了視頻生成的複雜性。通過Open-Sora，我們的目標是促進內容創作領域的創新、創意和包容性。 | 多模態基礎模型 |
| 2024年6月14日 | [Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209) | 大型語言模型可能會記住並重複其訓練數據，造成隱私和版權風險。為了減少記憶，我們引入了一個細微的修改，稱為金魚損失。在訓練期間，隨機選取的一部分字符將從損失計算中排除。這些被排除的字符不會被模型記住，從而防止從訓練集中逐字復制整個字符鏈。我們進行了廣泛的實驗，訓練了十億規模的Llama-2模型，包括預訓練和從頭開始訓練，並顯示出在記憶提取方面顯著的減少，對下游基準影響微乎其微。 | 新損失 |
| 2024年6月13日 | [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://arxiv.org/abs/2406.09415) | 這項工作不介紹新方法。相反，我們提出了一個有趣的發現，質疑現代計算機視覺架構中局部性的歸納偏見的必要性。具體來說，我們發現，普通Transformer可以通過將每個單獨的像素作為標記來運作，並取得高效能的結果。這與流行的視覺Transformer設計有實質性的不同，後者保持了從卷積網絡（ConvNet）繼承的局部鄰域歸納偏見（例如將每個16x16塊作為標記）。我們主要展示了像素作為標記在計算機視覺的三個研究任務中的有效性：物體分類的監督學習、自監督學習通過掩蔽自編碼，和圖像生成的擴散模型。雖然直接處理單個像素的計算成本較高，但我們認為社區在設計下一代計算機視覺神經架構時應該意識到這一令人驚訝的知識。 | 卷積網絡、Transformer |
| 2024年6月13日 | [Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models](https://arxiv.org/abs/2406.09403) | 人類通過畫圖來促進推理：我們在解決幾何問題時畫輔助線；在地圖上推理時標記和圈劃；用草圖來放大我們的想法並減輕我們有限容量的工作記憶。然而，這些行為在當前的多模態語言模型（LM）中缺失。當前的思維鏈和工具使用範式僅使用文本作為中間推理步驟。在這項工作中，我們引入了Sketchpad，一個給多模態LM提供視覺草圖板和工具來畫圖的框架。LM根據其畫出的視覺工件進行計劃和推理。與先前工作使用文本到圖像模型來使LM畫圖不同，Sketchpad允許LM使用線條、框、標記等畫圖，更接近於人類的草圖並更好地促進推理。Sketchpad還可以在草圖過程中使用專家視覺模型（例如，用物體檢測模型畫邊界框，用分割模型畫遮罩），進一步增強視覺感知和推理。我們在廣泛的數學任務（包括幾何、函數、圖形和象棋）和複雜視覺推理任務中進行實驗。Sketchpad在所有任務上均顯著提高了性能，相對於無草圖的強基線模型，數學任務平均增益12.7%，視覺任務增益8.6%。帶有Sketchpad的GPT-4o在所有任務上創下了新紀錄，包括V*Bench（80.3%）、BLINK空間推理（83.9%）和視覺對應（80.8%）。 | 多模態模型、提示工程 |
| 2024年6月12日 | [Multimodal Table Understanding](https://arxiv.org/abs/2406.08100) | 儘管以前的表理解方法，包括基於大型語言模型（LLM）的最新方法，已經取得了很大進展，但它們在很大程度上依賴於給定的表必須轉換為某種文本序列（如Markdown或HTML）以作為模型輸入。然而，在某些現實世界場景中很難獲得這樣的高質量文本表表示，表圖像則更容易獲得。因此，如何直接使用直觀的視覺信息來理解表格是一個關鍵且緊迫的挑戰，以開發更實用的應用程序。在本文中，我們提出了一個新問題，多模態表理解，模型需要基於給定的表圖像生成正確的響應以應對各種表相關請求。為了促進模型訓練和評估，我們構建了一個大規模數據集，名為MMTab，涵蓋了廣泛的表圖像、指令和任務。在此基礎上，我們開發了Table-LLaVA，一個通用的表格多模態大型語言模型（MLLM），在23個基準上顯著優於最近的開源MLLM基線，在內部和外部設置下均顯示出色的性能。 | 特定領域LLM |
| 2024年6月11日 | [TextGrad: Automatic "Differentiation" via Text](https://arxiv.org/abs/2406.07496v1) | AI正在經歷一場範式轉變，系統通過協同多個大型語言模型（LLM）和其他複雜組件取得了突破。結果，為複合AI系統開發原則性和自動化的優化方法成為了一個最重要的新挑戰。神經網絡在早期面臨類似的挑戰，直到反向傳播和自動微分通過使優化變得即插即用，徹底改變了該領域。受此啟發，我們引入了TextGrad，一個強大的框架，通過文本進行自動「微分」。TextGrad反向傳播LLM提供的文本反饋，以改進複合AI系統的各個組件。在我們的框架中，LLM提供豐富、通用的自然語言建議，以優化計算圖中的變量，從代碼片段到分子結構。TextGrad遵循PyTorch的語法和抽象，靈活且易於使用。它開箱即用，適用於各種任務，用戶僅提供目標函數而不需調整框架的組件或提示。我們展示了TextGrad在廣泛應用中的有效性和通用性，從問答和分子優化到放射治療計劃。在不修改框架的情況下，TextGrad將GPT-4o在Google-Proof問答中的零樣本準確率從51%提高到55%，在LeetCode-Hard編碼問題解決中的相對性能提高20%，改進提示以進行推理，設計具有良好計算機模擬結合的新藥物分子，並設計具有高特異性的放射治療計劃。TextGrad為加速下一代AI系統的開發奠定了基礎。 | 優化算法 |
| 2024年6月11日 | [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](https://arxiv.org/abs/2406.07138) | 最近，許多方法已被開發以延長預訓練大型語言模型（LLM）的上下文長度，但它們通常需要在目標長度（≫4K）進行微調，並且難以有效利用上下文中部的信息。為了解決這些問題，我們提出了連續-相對性指數與高斯中部（CREAM），通過操縱位置索引插值位置編碼。除了簡單外，CREAM在訓練上也非常高效：它僅需要在預訓練的上下文窗口（例如Llama 2-4K）進行微調，並可以將LLM延伸到更長的目標上下文長度（例如256K）。為了確保模型更專注於上下文中的信息，我們引入了截斷的高斯，在微調期間鼓勵從上下文中部進行抽樣，從而緩解長上下文LLM面臨的「中部丟失」問題。實驗結果顯示，CREAM成功地將LLM擴展到目標長度，適用於Llama2-7B的基礎版和聊天版，且效果穩定。我們的代碼即將公開。 | 上下文長度 |
| 2024年6月11日 | [Needle In A Multimodal Haystack](https://arxiv.org/abs/2406.07230) | 隨著多模態大型語言模型（MLLM）的快速發展，其評估變得越來越全面。然而，理解長篇多模態內容作為現實應用的基礎能力仍未充分探討。在本文中，我們提出了Needle In A Multimodal Haystack（MM-NIAH），這是第一個專門設計用於系統評估現有MLLM理解長篇多模態文檔能力的基準。我們的基準包括三種類型的評估任務：多模態檢索、計數和推理。在每個任務中，模型需要根據散佈在給定多模態文檔中的不同關鍵信息回答問題。評估領先的MLLM在MM-NIAH上的表現，我們觀察到現有模型在這些任務上仍有很大的改進空間，特別是在視覺中心的評估上。我們希望這項工作能為進一步研究長篇多模態文檔理解提供一個平台，並為MLLM的進步做出貢獻。 | 多模態模型 |
| 2024年6月11日 | [Estimating the Hallucination Rate of Generative AI](https://arxiv.org/abs/2406.07457) | 本文討論了用生成性AI進行上下文學習（ICL）的幻覺率估計。在ICL中，條件生成模型（CGM）根據給定數據集並基於該數據集進行預測。ICL的貝葉斯解釋假設CGM正在計算潛在參數和數據的未知貝葉斯模型的後驗預測分佈。從這一角度看，我們將幻覺定義為根據真實潛在參數概率低的生成預測。我們開發了一種新方法，能夠根據ICL問題（即CGM、數據集和預測問題）估計CGM生成幻覺的概率。我們的方法僅需要生成查詢和模型回應，並評估其回應的對數概率。我們在使用大型語言模型的合成迴歸和自然語言ICL任務上對我們的方法進行了實驗評估。 | 幻覺 |
| 2024年6月11日 | [Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524) | 雖然擴散模型在生成高質量圖像方面表現出色，但先前的研究報告說，擴散方法與自回歸方法在語言建模方面存在顯著性能差距。在這項工作中，我們表明，簡單的掩蔽離散擴散模型比以前認為的更有表現力。我們應用了一個有效的訓練策略，改善了掩蔽擴散模型的性能，並推導出一個簡化的Rao-Blackwell化目標，進一步提高性能。我們的目標具有簡單的形式——它是經典掩蔽語言建模損失的混合體——可以用於訓練僅有編碼器的語言模型，這些模型允許高效的採樣，包括可以像傳統語言模型一樣半自回歸地生成任意長度的文本。在語言建模基準上，應用了現代工程實踐的一系列掩蔽擴散模型在擴散模型中達到了新的最先進水平，接近自回歸的困惑度。我們在https://github.com/kuleshov-group/mdlm發布了我們的代碼。 | 擴散模型 |
| 2024年6月11日 | [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188) | 大型語言模型（LLM）對抗對抗性操作（如越獄攻擊）的魯棒性仍然是一個重大挑戰。在這項工作中，我們提出了一種增強LLM自我批判能力的方法，並通過對經過消毒的合成數據進行微調來進一步強化它。這是通過將一個外部批判模型與原始模型合併來實現的，從而加強自我批判能力並改善LLM對對抗性提示的響應魯棒性。我們的結果表明，合併和自我批判的結合可以顯著降低對手的攻擊成功率，從而為越獄攻擊提供了一個有前途的防禦機制。代碼、數據和模型在https://github.com/vicgalle/merging-self-critique-jailbreaks發布。 | 對抗性攻擊、越獄 |
| 2024年6月10日 | [NATURAL PLAN: Benchmarking LLMs on Natural Language Planning](https://arxiv.org/abs/2406.04520) | 我們介紹NATURAL PLAN，一個包含三個關鍵任務的現實計劃基準：行程計劃、會議計劃和日曆安排。我們將評估重點放在LLM的計劃能力上，通過提供來自Google Flights、Google Maps和Google Calendar等工具的輸出作為上下文，消除了需要評估LLM計劃的工具使用環境。我們觀察到NATURAL PLAN對最先進的模型來說是一個具有挑戰性的基準。例如，在行程計劃中，GPT-4和Gemini 1.5 Pro僅能達到31.1%和34.8%的解決率。我們發現隨著問題複雜性的增加，模型性能急劇下降：當有10個城市時，所有模型的表現都低於5%，顯示了最先進的LLM在自然語言計劃方面存在顯著差距。我們還在NATURAL PLAN上進行了廣泛的消融研究，以進一步說明自我修正、少樣本泛化和長上下文內的上下文計劃等方法在改進LLM計劃方面的（無效）性。 | 代理、計劃 |
| 2024年6月7日 | [SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals](https://arxiv.org/abs/2406.04784) | 由大型語言模型（LLM）驅動的語言代理在遊戲和編程等領域作為決策工具越來越有價值。然而，這些代理在沒有詳細指示的情況下達成高級目標以及適應反饋延遲的環境方面經常面臨挑戰。在本文中，我們提出了SelfGoal，一種旨在增強代理在有限的人類先驗和環境反饋下達成高級目標能力的新方法。SelfGoal的核心概念是通過與環境的互動，自適應地將高級目標分解為更實際的子目標，並識別最有用的子目標，逐步更新這一結構。實驗結果顯示，SelfGoal顯著增強了語言代理在各種任務中的性能，包括競爭性、合作性和延遲反饋環境。 | 代理、任務分解 |
| 2024年6月7日 | [CRAG -- Comprehensive RAG Benchmark](https://arxiv.org/abs/2406.04744) | 檢索增強生成（RAG）最近作為一種有前途的解決方案出現，以緩解大型語言模型（LLM）在知識缺乏方面的不足。然而，現有的RAG數據集並未充分代表現實問答（QA）任務的多樣性和動態性。為了彌補這一差距，我們引入了綜合RAG基準（CRAG），這是一個包含4409個問答對和模擬Web和知識圖（KG）搜索的API的事實問答基準。CRAG旨在涵蓋五個領域和八個問題類別的多樣化問題，反映了從受歡迎到長尾的不同實體受歡迎程度和從幾年到幾秒的時間動態。我們在這一基準上的評估突顯了完全可靠的QA的差距。最先進的LLM在CRAG上達到的準確率≤34%，簡單添加RAG僅將準確率提高到44%。最先進的行業RAG解決方案只回答了63%的問題，沒有任何幻覺。CRAG還顯示，回答高動態性、低受歡迎度或高複雜性的事實問題的準確率更低，這表明了未來研究方向。CRAG基準為KDD Cup 2024挑戰奠定了基礎，在前50天內吸引了數千名參與者和提交。我們承諾維護CRAG，以服務於研究社區，推動RAG解決方案和通用QA解決方案的進步。 | RAG、基準 |
| 2024年6月7日 | [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692) | 最近在大型語言模型（LLM）方面的進展展示了其在自然語言理解和生成任務上的顯著能力。隨著LLM數量的增加，如何利用多個LLM的集體專長是一個令人興奮的開放方向。為此，我們提出了一種新的方法，通過混合代理（MoA）方法利用多個LLM的集體優勢。在我們的方法中，我們構建了一個分層MoA架構，每層包含多個LLM代理。每個代理在生成其回應時，都會將來自上一層代理的所有輸出作為輔助信息。MoA模型在AlpacaEval 2.0、MT-Bench和FLASK上達到最先進的性能，超過了GPT-4 Omni。例如，我們的MoA僅使用開源LLM，在AlpacaEval 2.0上以顯著差距領先，達到65.1%的得分，相比之下GPT-4 Omni為57.5%。 | 代理、多代理 |
| 2024年6月6日 | [AgentGym: Evolving Large Language Model-based Agents across Diverse Environments](https://arxiv.org/abs/2406.04151) | 構建能夠處理多樣任務並在不同環境中自我進化的通用代理是AI社區的長期目標。大型語言模型（LLM）被認為是構建此類代理的有希望的基礎，因其具有廣泛的能力。目前的方法要麼讓基於LLM的代理模仿專家提供的軌跡步驟，需要人類監督，難以擴展，限制了環境探索；要麼讓代理在隔離環境中探索和學習，導致僅限於特定環境的專家代理。在本文中，我們邁出了構建具備自我進化能力的一般能力LLM代理的第一步。我們確定了三個成分：1）供代理探索和學習的多樣環境，2）裝備代理基本能力和先驗知識的軌跡集，3）一種有效且可擴展的進化方法。我們提出了AgentGym，一個新的框架，具有多樣的環境和任務，供廣泛、實時、單格式和並行的代理探索。AgentGym還包括一個擴展的指令數據庫、一個基準套件和跨環境的高質量軌跡。接下來，我們提出了一種新方法，AgentEvol，探索代理在見過的數據之外的自我進化潛力。實驗結果顯示，進化的代理在各任務和環境中達到與最先進模型相當的結果。我們發布了AgentGym套件，包括平台、數據集、基準、檢查點和算法實現。 | 代理 |
| 2024年6月6日 | [Are We Done with MMLU?](https://arxiv.org/abs/2406.04127) | 可能還沒有。我們識別並分析了流行的Massive Multitask Language Understanding（MMLU）基準中的錯誤。儘管MMLU被廣泛採用，我們的分析顯示了許多真實錯誤，這些錯誤掩蓋了LLM的真實能力。例如，我們發現病毒學子集中57%的問題包含錯誤。為了解決這一問題，我們引入了一個全面的框架，使用新的錯誤分類法識別數據集錯誤。然後，我們創建了MMLU-Redux，這是一個包含30個MMLU學科中3000個手動重新標註問題的子集。使用MMLU-Redux，我們展示了與最初報告的模型性能指標顯著不一致。我們的結果強烈主張修訂MMLU的錯誤問題，以提高其未來作為基準的實用性和可靠性。因此，我們開放MMLU-Redux以供進一步標註https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux。 | 評估、任務基準 |
| 2024年6月6日 | [GenAI Arena: An Open Evaluation Platform for Generative Models](https://arxiv.org/abs/2406.04485) | 生成性AI在圖像和視頻生成領域取得了顯著進展。這些進展由創新的算法、架構和數據推動。然而，生成模型的快速增殖突顯出一個關鍵差距：缺乏可靠的評估指標。當前的自動評估（如FID、CLIP、FVD等）經常未能捕捉生成輸出的微妙質量和用戶滿意度。本文提出了一個開放平台GenAI-Arena，用於評估不同的圖像和視頻生成模型，用戶可以積極參與評估這些模型。通過利用集體用戶反饋和投票，GenAI-Arena旨在提供更民主和準確的模型性能度量。它涵蓋三個競技場，分別為文本到圖像生成、文本到視頻生成和圖像編輯。目前，我們涵蓋了27個開源生成模型。GenAI-Arena已運行四個月，收集了來自社區的6000多票。我們描述了我們的平台，分析了數據，並解釋了排名模型的統計方法。為進一步促進基於模型的評估指標研究，我們釋出了一個清理版本的我們的偏好數據，用於三個任務，稱為GenAI-Bench。我們提示現有的多模態模型，如Gemini、GPT-4o，以模仿人類投票。我們計算了模型投票與人類投票的相關性，以了解其評判能力。我們的結果顯示，現有的多模態模型在評估生成的視覺內容方面仍然落後，最好的模型GPT-4o在質量子分數中的皮爾遜相關性僅為0.22，其他方面的表現像隨機猜測。 | 評估 |
| 2024年6月6日 | [Scaling and evaluating sparse autoencoders](https://arxiv.org/abs/2406.04093) | 稀疏自編碼器提供了一種有前途的無監督方法，通過重構稀疏瓶頸層的激活來從語言模型中提取可解釋特徵。由於語言模型學習了許多概念，自編碼器需要非常大才能恢復所有相關特徵。然而，研究自編碼器擴展的屬性很困難，因為需要平衡重構和稀疏性目標，並且存在無效潛在變量。我們提出使用k稀疏自編碼器[2013]來直接控制稀疏性，簡化調優並改善重構-稀疏性前沿。此外，我們發現了一些修改，即使在我們嘗試的最大規模下也能減少無效潛在變量。使用這些技術，我們發現了自編碼器大小和稀疏性的清晰縮放定律。我們還引入了幾個新的特徵質量評估指標，基於假設特徵的恢復、激活模式的可解釋性和下游效果的稀疏性。這些指標隨著自編碼器大小的增加而一般改善。為了展示我們方法的可擴展性，我們在GPT-4激活上訓練了一個1600萬潛在變量的自編碼器，處理了400億個字符。我們釋出訓練代碼和開源模型的自編碼器以及一個可視化工具。 | LLM架構 |
| 2024年6月6日 | [Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](https://arxiv.org/abs/2406.04271) | 我們介紹了Buffer of Thoughts（BoT），這是一種新穎且多功能的思維增強推理方法，用於提高大型語言模型（LLM）的準確性、效率和魯棒性。具體來說，我們提出了元緩衝區，用於存儲一系列來自各種任務問題解決過程中提取的高級思維，即思維模板。然後對於每個問題，我們檢索相關的思維模板並自適應地實例化它以進行具體的推理結構。為了保證可擴展性和穩定性，我們進一步提出了緩衝區管理器，以動態更新元緩衝區，從而隨著解決的任務增多而增強元緩衝區的容量。我們在10個具有挑戰性的推理密集型任務上進行了廣泛實驗，並在先前的SOTA方法上取得了顯著的性能提升：在24遊戲中提升11%，在幾何形狀中提升20%，在一招將軍中提升51%。進一步的分析顯示了BoT的優越泛化能力和模型魯棒性，同時平均只需多查詢方法（如思維樹/圖）的12%成本。值得注意的是，我們發現Llama3-8B+BoT有潛力超越Llama3-70B模型。 | RAG、知識整合 |
| 2024年6月5日 | [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](https://arxiv.org/abs/2406.06592) | 複雜的多步推理任務，如解數學問題或生成代碼，即使是最先進的LLM也面臨重大挑戰。使用結果獎勵模型（ORM）驗證LLM輸出是一種旨在提高LLM推理性能的標準推理時間技術。然而，這對於具有長或多跳推理鏈的推理任務仍然不足，因為中間結果既不被適當獎勵也不被懲罰。過程監督通過在推理過程中分配中間獎勵來解決這一限制。迄今為止，用於收集過程監督數據的方法依賴於人類標註或逐步蒙特卡洛估計，這兩者都難以擴展，從而阻礙了這一技術的廣泛應用。為應對這一挑戰，我們提出了一種新的分而治之的蒙特卡洛樹搜索（MCTS）算法，稱為OmegaPRM，用於高效收集高質量的過程監督數據。該算法通過二分搜索迅速識別推理鏈中的第一個錯誤，並平衡正面和負面示例，從而保證了效率和質量。結果，我們能夠收集超過150萬個過程監督註釋來訓練過程獎勵模型（PRM）。利用這種完全自動化的過程監督和加權自一致性算法，我們提高了指令調優Gemini Pro模型的數學推理性能，在MATH基準上達到69.4%的成功率，相對於基礎模型性能提升了36%。此外，整個過程不需要任何人工干預，使我們的方法在財務和計算上都比現有方法更具成本效益。 | 數學推理 |
| 2024年6月5日 | [SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales](https://arxiv.org/abs/2405.20974) | 大型語言模型（LLM）經常生成不準確或虛構的信息，且通常無法表示其信心，這限制了其更廣泛的應用。先前的工作通過直接或自一致性提示來引出LLM的信心，或構建特定數據集進行監督微調。提示方法性能較差，訓練方法僅限於二元或不準確的組級信心估計。在本文中，我們提出了先進的SaySelf，一個訓練框架，旨在教LLM表達更準確的細粒度信心估計。此外，除了信心分數外，SaySelf還啟動了引導LLM生成自我反思理由的過程，這些理由清楚地識別其參數知識中的差距並解釋其不確定性。這是通過使用LLM自動總結具體知識的不確定性來實現的，基於對多個抽樣推理鏈中的不一致性分析，並將結果數據用於監督微調。此外，我們利用具有精心設計的獎勵函數的強化學習來校準信心估計，激勵LLM提供準確的高信心預測，並懲罰錯誤輸出的過度信心。在內部和外部數據集上的實驗結果顯示，SaySelf在減少信心校準誤差和保持任務性能方面的有效性。我們展示了生成的自我反思理由是合理的，並且可以進一步促進校準。 | LLM訓練、LLM挑戰 |
| 2024年6月4日 | [Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507) | 圖像生成擴散模型的主要關注點是圖像質量、結果的變異量以及結果與給定條件（例如類別標籤或文本提示）的對齊程度。流行的無分類器引導方法使用無條件模型來引導有條件模型，從而在提高提示對齊和圖像質量的同時降低變異量。這些效果似乎內在地糾纏在一起，因此難以控制。我們驚訝地發現，通過使用一個較小、訓練較少的模型版本而非無條件模型來引導生成，可以實現圖像質量和變異量的解耦控制。這導致了ImageNet生成的顯著改進，使用公開可用的網絡在64x64和512x512圖像生成上分別創下了1.01和1.25的FID紀錄。此外，該方法也適用於無條件擴散模型，顯著提高了其質量。 | 擴散模型 |
| 2024年6月4日 | [To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543) | 我們探討大型語言模型（LLM）中的不確定性量化，目標是識別給定查詢的回應中不確定性較大的情況。我們同時考慮了知識性不確定性和不可避免的不確定性，前者來自於對真實情況（如事實或語言）的知識不足，後者來自於不可減少的隨機性（如多種可能的答案）。具體來說，我們推導出了一個信息論指標，允許可靠地檢測何時只有知識性不確定性較大，此時模型輸出不可靠。這個條件可以僅基於模型輸出的結果計算，通過一些基於先前回應的特別迭代提示進行計算。這種量化方法允許在單一和多重答案回應中檢測幻覺（即知識性不確定性較高的情況）。這與許多標準不確定性量化策略（如閾值化回應的對數可能性）形成對比，後者無法在多重答案情況下檢測幻覺。我們進行了一系列實驗，展示了我們公式的優勢。此外，我們的調查揭示了LLM對給定輸出分配的概率可以通過迭代提示放大，這可能具有獨立的興趣。 | 幻覺、不確定性估計 |
| 2024年6月3日 | [Self-Improving Robust Preference Optimization](https://arxiv.org/abs/2406.01660) | 在線和離線RLHF方法如PPO和DPO在對齊AI與人類偏好方面取得了巨大成功。儘管它們取得了成功，現有方法存在一個根本問題，即其最佳解決方案高度依賴於任務（即對分佈外（OOD）任務不具有魯棒性）。在本文中，我們通過提出自我改進魯棒偏好優化SRPO來解決這一挑戰，這是一個實用且數學上合理的離線RLHF框架，對任務的變化完全魯棒。SRPO的關鍵思想是將從人類偏好中學習的問題視為一個自我改進過程，數學上表示為一個min-max目標，旨在聯合優化自我改進策略和生成策略，以對抗性方式。該優化問題的解決方案獨立於訓練任務，因此對其變化具有魯棒性。我們然後展示該目標可以重新表述為一個非對抗性的離線損失，可以使用標準監督優化技術在規模上進行優化，無需獎勵模型和在線推理。我們通過AI勝率（WR）對人類（GOLD）完成的評估展示了SRPO的有效性。特別是，當SRPO在OOD XSUM數據集上進行評估時，經過5次自我修訂後，它比著名的DPO高出15%，達到了90%的WR。 | 優化、對齊 |
| 2024年6月3日 | [Towards Scalable Automated Alignment of LLMs: A Survey](https://arxiv.org/abs/2406.01252) | 對齊是構建符合人類需求的大型語言模型（LLM）的最關鍵步驟。隨著LLM的快速發展逐漸超越人類能力，基於人工標註的傳統對齊方法越來越無法滿足可擴展性的需求。因此，迫切需要探索新的自動化對齊信號來源和技術方法。在本文中，我們系統地回顧了最近出現的自動化對齊方法，試圖探索如何在LLM能力超越人類的情況下實現有效、可擴展的自動化對齊。具體來說，我們將現有的自動化對齊方法分為四大類，並討論每一類的現狀和潛在發展。此外，我們探索了自動化對齊的基本機制，並討論了從對齊的基本作用來看，使自動化對齊技術可行和有效的關鍵因素。 | 對齊 |
| 2024年6月2日 | [Show, Don't Tell: Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888) | 語言模型被對齊以模仿許多人的集體聲音，導致輸出與特定人群不一致。通過監督微調或RLHF可以將LLM引導離開通用輸出，但需要大規模數據集來應對新臨時任務。我們認為，可以通過利用少量（<10）演示作為反饋來將LLM對齊到特定設置。我們的方法，演示迭代任務優化（DITTO），直接將語言模型輸出對齊到用戶的演示行為。基於在線模仿學習的理念，DITTO廉價地生成在線比較數據，將用戶的演示視為優於LLM及其中間檢查點的輸出。我們評估了DITTO在新聞文章、電子郵件和博客文章等領域中學習細粒度風格和任務對齊的能力。此外，我們進行了一項用戶研究，收集了參與者（N=16）的各種演示。在我們的基準和用戶研究中，我們發現DITTO的勝率平均超過了少樣本提示、監督微調和其他自我遊戲方法19個百分點。通過直接將演示作為反饋，DITTO提供了一種有效定制LLM的新方法。 | 對齊 |
| 2024年6月1日 | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) | 雖然Transformer一直是語言建模成功背後的主要架構，狀態空間模型（SSM）如Mamba最近被證明在小到中等規模上匹配或超越Transformer。我們發現這些模型家族實際上非常密切相關，並開發了一個豐富的理論框架，將SSM和各種注意力變體通過結構化半分離矩陣的各種分解聯繫起來。我們的狀態空間對偶（SSD）框架允許我們設計一個新的架構（Mamba-2），其核心層是Mamba選擇性SSM的改進，速度提高2-8倍，同時在語言建模上與Transformer競爭。 | LLM架構 |
| 2024年6月1日 | [Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning](https://arxiv.org/abs/2406.00392) | 文化積累驅動了人類歷史上能力的開放性和多樣性進步。通過結合個人探索和跨代信息傳遞，它建立了一個不斷擴展的知識和技能體系。儘管在人類中取得了廣泛的成功，人工學習代理積累文化的能力仍未得到充分探索。特別是，強化學習的方法通常只在單一生命週期內追求改進。現有的代際算法未能捕捉到文化積累的開放性、突現性，這允許個體在創新和模仿之間進行權衡。在先前展示的強化學習代理進行社會學習的基礎上，我們發現，平衡這一點與獨立學習的訓練設置使文化積累成為可能。這些積累的代理超越了那些在單一生命週期內訓練的代理，累積經驗相同。我們通過構建兩個模型，分別基於兩種不同的世代概念來探索這一積累：基於情景世代，通過上下文學習進行積累；基於訓練時間世代，通過權重學習進行積累。上下文和權重文化積累可以解釋為分別類似於知識和技能積累。據我們所知，這項工作首次提出了在強化學習中實現文化積累的通用模型，開闢了朝著更開放性學習系統的新途徑，並為模擬人類文化提供了新的機會。 | 文化適應 |

