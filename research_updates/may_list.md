| 日期 | 名稱 | 摘要 | 主題 |
| --- | --- | --- | --- |
| 2024年5月31日 | [LLMs achieve adult human performance on higher-order theory of mind tasks](https://arxiv.org/pdf/2405.18870) | 本論文探討大型語言模型（LLMs）在高階心智理論（ToM）任務中達到成人水準的程度；心智理論指的是人類以遞迴方式推理多種心理和情緒狀態的能力（例如：我認為你相信她知道）。本研究在之前工作的基礎上，介紹了一套手寫測試套件——多階心智理論問答（Multi-Order Theory of Mind Q&A），並用它來比較五個 LLMs 的表現與新收集的成人基準。結果顯示，GPT-4 和 Flan-PaLM 在整體 ToM 任務中達到或接近成人水準，且 GPT-4 在第六階推理中超越成人表現。我們的結果表明，模型大小和微調在實現 ToM 能力方面存在相互作用，表現最好的 LLMs 已經發展出通用的 ToM 能力。考慮到高階 ToM 在多種合作和競爭行為中的角色，這些發現對用戶面向的 LLM 應用有重大影響。 | 心智理論 |
| 2024年5月30日 | [JINA CLIP: Your CLIP Model Is Also Your Text Retriever](https://arxiv.org/pdf/2405.20204) | 對比語言-圖像預訓練（CLIP）被廣泛用於將圖像和文本對齊到共同嵌入空間，將它們映射為固定大小的向量。這些模型對多模態信息檢索和相關任務至關重要。然而，CLIP 模型在僅文本任務中的表現通常不如專門的文本模型，這會導致信息檢索系統在文本和多模態任務中保留單獨的嵌入和模型，造成效率低下。我們提出了一種新穎的多任務對比訓練方法來解決這一問題，並用它訓練 jina-clip-v1 模型，在文本-圖像和文本-文本檢索任務中實現最先進的性能。 | 多模態模型 |
| 2024年5月30日 | [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](https://arxiv.org/pdf/2405.19888) | 大型語言模型（LLMs）的崛起促使了基於 LLMs 的應用（即 AI 代理或副駕駛）的興起，這是一種結合 LLM 和傳統軟件的新軟件範式。來自不同租戶的多樣化 LLM 應用可以設計複雜的工作流程，使用多個 LLM 請求完成一個任務。然而，他們不得不使用當前公共 LLM 服務提供的過於簡化的請求級別 API，失去了基本的應用級別信息。公共 LLM 服務不得不盲目優化單個 LLM 請求，導致 LLM 應用的端到端性能次優。本文介紹了 Parrot，一個專注於基於 LLM 應用的端到端體驗的 LLM 服務系統。Parrot 提出了語義變量，一種統一的抽象概念，用於將應用級別的知識暴露給公共 LLM 服務。語義變量註釋了請求中輸入/輸出變量，並在連接多個 LLM 請求時創建數據管道，提供了一種自然的方式來編程 LLM 應用。將語義變量暴露給公共 LLM 服務允許它執行常規數據流分析，以揭示多個 LLM 請求之間的相關性。這種相關性為 LLM 應用的端到端性能打開了一個全新的優化空間。廣泛的評估表明，Parrot 可以為 LLM 應用的熱門和實際用例實現高達一個數量級的改進。 | LLM 代理 |
| 2024年5月30日 | [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](https://arxiv.org/pdf/2405.20541) | 本研究探討小型語言模型是否能夠確定大型文本數據集中高品質的子集，以提升大型語言模型的性能。雖然現有研究表明基於大型模型困惑度的剪枝可以產生高品質數據，我們探討了使用小型模型進行困惑度剪枝以及數據領域組成對剪枝的影響。我們證明了在多種數據集組成中，基於困惑度的預訓練數據剪枝可以顯著提升下游任務性能：基於 1.25 億參數模型計算的困惑度剪枝可以提升 30 億參數模型的下游任務平均性能高達 2.04，並實現預訓練步驟減少 1.45 倍以達到相應的基準性能。此外，我們證明基於困惑度的數據剪枝在過度訓練和數據受限的情況下也能提升下游性能。 | 小型語言模型 |
| 2024年5月30日 | [GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning](https://arxiv.org/pdf/2405.20139) | 知識圖譜（KGs）以三元組（主體、關係、賓語）的形式表示人工構建的事實知識，這些三元組共同形成一個圖。基於知識圖譜的問答（KGQA）是指將問題的推理基礎定位於知識圖譜提供的資訊來回答自然問題。大型語言模型（LLMs）因其理解自然語言的非凡能力而成為問答任務的最先進模型。另一方面，圖神經網絡（GNNs）廣泛用於 KGQA，因為它們能處理知識圖譜中存儲的複雜圖形信息。本研究提出 GNN-RAG，一種結合 GNN 和 LLM 的檢索增強生成（RAG）方法。首先，GNN 對稠密的知識圖譜子圖進行推理，檢索出問題的答案候選。其次，從知識圖譜中提取出問題實體和答案候選之間的最短路徑，以表示知識圖譜的推理路徑。這些路徑被語言化並作為輸入給 LLM 進行 RAG 推理。在我們的 GNN-RAG 框架中，GNN 作為稠密子圖推理器提取有用的圖信息，而 LLM 則利用其自然語言處理能力完成最終的 KGQA。此外，我們還開發了一種檢索增強（RA）技術，以進一步提升 GNN-RAG 的 KGQA 性能。實驗結果表明，GNN-RAG 在兩個廣泛使用的 KGQA 基準（WebQSP 和 CWQ）中達到最先進的性能，超越或匹配 GPT-4 在多跳和多實體問題上的表現，超越競爭方法 8.9-15.5 個百分點。我們在 https://github.com/cmavro/GNN-RAG 提供代碼和 KGQA 結果。 | 基於知識圖譜的 RAG |
| 2024年5月29日 | [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](https://arxiv.org/pdf/2405.19332) | 偏好優化，特別是通過人類反饋強化學習（RLHF），在使大型語言模型（LLMs）遵循人類意圖方面取得了重大成功。與使用固定數據集的離線對齊不同，從人類或 AI 收集模型生成的在線反饋通常會通過迭代過程產生更強大的獎勵模型和更好對齊的 LLMs。然而，實現全球準確的獎勵模型需要系統地探索生成多樣的回應，以涵蓋自然語言的廣闊空間。僅從標準的最大化獎勵的 LLM 隨機取樣不足以滿足這一要求。為了解決這一問題，我們提出了一種雙層目標，樂觀地偏向潛在高獎勵的回應，以積極探索分佈外的區域。通過使用重新參數化獎勵函數解決內層問題，結果算法（稱為自我探索語言模型，SELM）不需要單獨的 RM，並通過簡單的目標迭代更新 LLM。與直接偏好優化（DPO）相比，SELM 目標減少了對未見外推的無差別偏好，並提高了探索效率。我們的實驗結果顯示，在 Zephyr-7B-SFT 和 Llama-3-8B-Instruct 模型上進行微調時，SELM 在指令跟隨基準（如 MT-Bench 和 AlpacaEval 2.0）以及各種不同設置的標準學術基準上顯著提升性能。我們的代碼和模型可在 https://github.com/shenao-zhang/SELM 獲得。 | 對齊，偏好優化 |
| 2024年5月28日 | [OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143) | 隨著大型語言模型（LLMs）不斷按照縮放定律增長，來自人類反饋的強化學習（RLHF）因其卓越的表現而受到廣泛關注。然而，與單一模型的預訓練或微調不同，擴展 RLHF 以訓練大型語言模型在四個模型之間的協調方面存在挑戰。我們推出了 OpenRLHF，一個開源框架，實現高效的 RLHF 擴展。與現有的 RLHF 框架將四個模型共置於同一 GPUs 上不同，OpenRLHF 使用 Ray、vLLM 和 DeepSpeed 重新設計超過 70B 參數模型的調度，利用改進的資源利用率和多樣化的訓練方法。OpenRLHF 與 Hugging Face 無縫集成，提供開箱即用的解決方案，包含優化的算法和啟動腳本，確保用戶友好性。OpenRLHF 實施了 RLHF、DPO、拒絕取樣和其他對齊技術。賦能最先進的 LLM 開發，OpenRLHF 的代碼可在 https://github.com/OpenLLMAI/OpenRLHF 獲得。 | RLHF，工具包 |
| 2024年5月28日 | [LLAMA-NAS: EFFICIENT NEURAL ARCHITECTURE SEARCH FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2405.18377) | 現代大型語言模型（LLMs）在解決自然語言處理、複雜推理、情感分析等任務方面表現卓越，促使其廣泛應用。然而，這些能力伴隨著極高的記憶體和計算成本，限制了 LLMs 在大多數硬件平台上的使用。為了解決這一問題，我們提出了一種基於 LLaMA2-7B 的 Pareto 最優網絡架構搜索方法。具體來說，我們只對 LLaMA2-7B 進行一次微調，然後應用基因算法搜索找到較小、計算量較低的網絡架構。我們顯示，對於某些標準基準任務，預訓練的 LLaMA2-7B 網絡過於龐大和複雜。我們具體展示了模型大小減少 1.5 倍，吞吐量加速 1.3 倍，而準確性下降可忽略不計。除了找到更小、更高效的網絡架構外，我們的方法還比某些剪枝或稀疏化技術更有效。此外，我們展示了量化與我們的方法互補，網絡的大小和複雜性可以通過量化進一步減少。我們認為，我們的工作為自動創建可以在較便宜、更易獲得的硬件平台上使用的 LLMs 提供了一條途徑。 | 神經架構搜索，模型大小減少 |
| 2024年5月28日 | [Don't Forget to Connect! Improving RAG with Graph-based Reranking](https://arxiv.org/pdf/2405.18414) | 檢索增強生成（RAG）顯著提高了大型語言模型（LLM）回應的性能，通過將生成的內容與現有文件的上下文結合。這些系統在文件與問題上下文明確相關時效果很好。但當文件僅有部分信息或與上下文的連接不明顯時怎麼辦？我們應該如何推理文件之間的連接？本研究旨在回答這兩個核心問題。我們介紹 G-RAG，一個基於圖神經網絡（GNN）的重排序器，位於 RAG 中的檢索器和讀取器之間。該方法結合文件之間的連接和語義信息（通過抽象意圖表示圖），提供上下文知識的重排序器。G-RAG 優於最先進的方法，同時具有較小的計算資源佔用。此外，我們評估了 PaLM 2 作為重排序器的性能，發現其顯著劣於 G-RAG。這一結果強調了即使使用大型語言模型，重排序對於 RAG 的重要性。 | 基於圖的 RAG |
| 2024年5月27日 | [Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models](https://arxiv.org/pdf/2405.15574) | 大型語言和視覺模型（LLVMs）的快速發展得益於視覺指令調整的進展。最近，開源 LLVMs 收集了高品質的視覺指令調整數據集，並使用了附加的視覺編碼器或多個計算機視覺模型，以縮小與強大的閉源 LLVMs 之間的性能差距。這些進步歸功於多方面的信息需求，包括基本圖像理解、關於常識和非物體概念（如圖表、圖示、符號和數學問題）的現實知識，以及解決複雜問題的逐步過程。基於多方面信息，我們提出了一種新型高效 LLVM，基於 Mamba 的推理遍歷（Meteor），利用多方面推理提升理解和回答能力。為了嵌入包含豐富信息的長推理，我們採用 Mamba 架構，能以線性時間複雜度處理順序數據。我們引入了一種新的推理遍歷概念，有助於高效嵌入推理。隨後，主幹多模態語言模型（MLM）在推理幫助下生成答案。通過這些步驟，Meteor 在多個需要多方面能力的評估基準中顯著提升視覺語言性能，而不需擴展模型大小或使用附加的視覺編碼器和計算機視覺模型。代碼可在 https://github.com/ByungKwanLee/Meteor 獲得。 | 狀態空間模型，多模態模型 |
| 2024年5月27日 | [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247) | 隨著大型語言模型（LLMs）的普及，許多嘗試將它們擴展到視覺領域的研究不斷出現。從擁有能引導我們穿越陌生環境的視覺助手，到僅使用高級文本描述生成圖像的生成模型，視覺語言模型（VLM）的應用將顯著影響我們與技術的關係。然而，為了提高這些模型的可靠性，還有許多挑戰需要解決。雖然語言是離散的，但視覺在更高維度的空間中演變，概念不能總是容易離散化。為了更好地理解視覺映射到語言的機制，我們提供了這本關於 VLMs 的入門指南，希望能幫助任何想進入這一領域的人。首先，我們介紹了 VLMs 是什麼、它們如何工作以及如何訓練它們。然後，我們介紹和討論了評估 VLMs 的方法。儘管本研究主要關注將圖像映射到語言，我們也討論了將 VLMs 擴展到視頻的可能性。 | 多模態模型，調查研究 |
| 2024年5月27日 | [Matryoshka Multimodal Models](https://arxiv.org/pdf/2405.17430) | 大型多模態模型（LMMs）如 LLaVA 在視覺語言推理方面表現強勁。這些模型首先將圖像嵌入大量視覺標記，然後將它們輸入大型語言模型（LLM）。然而，這種設計在高密度視覺場景如高分辨率圖像和視頻中產生過多的標記，導致效率低下。儘管存在標記修剪和合併方法，它們產生的單一長度輸出無法在信息密度和效率之間提供靈活的權衡。受俄羅斯套娃的啟發，我們提出了 M3：俄羅斯套娃多模態模型，學習將視覺內容表示為多層次的視覺標記，捕捉多種粗細粒度的信息。我們的方法為 LMMs 提供了幾個獨特的好處：（1）可以在推理期間根據內容的複雜性或簡單性調整表示圖像的標記數量；（2）提供了一個框架來分析現有數據集所需的粒度，我們發現 COCO 風格的基準僅需約 9 個視覺標記即可獲得與使用所有 576 個標記相似的準確性；（3）我們的方法為探索性能和視覺標記長度之間的最佳權衡提供了基礎，我們的研究顯示當前固定規模表示和理想上限之間存在較大差距。 | 多模態模型 |
| 2024年5月27日 | [Trans-LoRA: towards data-free Transferable Parameter Efficient Finetuning](https://arxiv.org/pdf/2405.17258) | 低秩適配器（LoRA）及其變體是流行的參數高效微調（PEFT）技術，能在僅需少量額外參數的情況下接近全模型微調的性能。這些額外的 LoRA 參數專門適應基礎模型。當基礎模型需要棄用並更換新模型時，所有相關的 LoRA 模塊都需要重新訓練。這種重新訓練需要訪問用於訓練原始基礎模型的 LoRA 的數據。這對於商業雲應用特別麻煩，因為 LoRA 模塊和基礎模型由服務提供商託管，可能無法託管專有的客戶任務數據。為了解決這一挑戰，我們提出了 Trans-LoRA，一種無損、幾乎無數據轉移 LoRA 模塊到新模型的方法。我們的方法依賴於合成數據來轉移 LoRA 模塊。我們使用大型語言模型設計了一個合成數據生成器，以近似觀察到的任務數據子集的數據生成過程。在生成的合成數據集上訓練，將 LoRA 模塊轉移到新模型。我們展示了我們的方法在 LLaMA 和 Gemma 模型家族上的有效性。我們的方法在多種任務上實現了無損（多數情況下有改進）的 LoRA 轉移，並在相同和不同基礎模型家族以及不同 PEFT 方法之間進行轉移。 | PEFT 方法，微調 |
| 2024年5月26日 | [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/pdf/2405.00675) | 傳統的從人類反饋中進行強化學習（RLHF）方法依賴於參數模型，如 Bradley-Terry 模型，這些方法在捕捉人類偏好的非傳遞性和非理性方面表現不佳。最近的進展表明，直接處理偏好概率可以更準確地反映人類偏好，從而使語言模型對齊更靈活和準確。在本文中，我們提出了一種基於自我博弈的語言模型對齊方法，將問題視為一個恆和的雙人遊戲，目的是找到納什均衡策略。我們的方法稱為自我博弈偏好優化（SPPO），通過迭代策略更新來逼近納什均衡，並享有理論收斂保證。我們的方法能有效地增加選擇回應的對數似然，並減少被拒絕回應的對數似然，這是對稱成對損失（如直接偏好優化（DPO）和身份偏好優化（IPO））無法輕易實現的。在實驗中，我們使用來自 UltraFeedback 數據集的 60k 提示（無回應）微調 Mistral-7B-Instruct-v0.2 模型，發現 SPPO 在長度控制的 AlpacaEval 2.0 上的勝率達到 28.53%，超過 GPT-4-Turbo，並在 MT-Bench 和 Open LLM Leaderboard 上表現優於 DPO 和 IPO。值得注意的是，SPPO 的強大性能是在沒有來自 GPT-4 或其他更強語言模型的額外外部監督（如回應、偏好等）的情況下實現的。 | 對齊，優化 |
| 2024年5月23日 | [Not All Language Model Features Are Linear](https://arxiv.org/pdf/2405.14860) | 最近的研究提出了線性表示假設：語言模型通過在激活空間中操縱一維概念表示（「特徵」）來執行計算。相比之下，我們探討了一些語言模型表示是否天生是多維的。我們首先基於它們是否可以分解為獨立或不共現的低維特徵，提出了一個嚴格的不可約多維特徵定義。基於這些定義，我們設計了一種可擴展的方法，使用稀疏自編碼器自動發現 GPT-2 和 Mistral 7B 中的多維特徵。這些自動發現的特徵包括令人驚訝的可解釋例子，例如代表一周七天和一年十二個月的圓形特徵。我們確定了這些圓形特徵在涉及模運算的計算問題中被用來解決與一周七天和一年十二個月相關的任務。最後，我們通過干預實驗提供了這些圓形特徵在這些任務中確實是計算的基本單位的證據，並在 Mistral 7B 和 Llama 3 8B 上發現了更多圓形表示。 | 線性表示分析 |
| 2024年5月23日 | [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](https://arxiv.org/pdf/2405.14129) | 多模態大型語言模型（MLLMs）被廣泛認為是探索通用人工智能（AGI）的關鍵。MLLMs 的核心在於實現跨模態對齊。為了實現這一目標，目前的 MLLMs 通常遵循兩階段訓練範式：預訓練階段和指令調整階段。儘管取得了成功，但這些模型在對齊能力建模方面存在不足。首先，在預訓練階段，模型通常假設所有圖文對都是均勻對齊的，但實際上不同圖文對之間的對齊程度是不一致的。其次，目前用於微調的指令包含各種任務，不同任務的指令通常需要不同級別的對齊能力，但現有的 MLLMs 忽略了這些差異化的對齊需求。為了解決這些問題，我們提出了一種新的多模態大型語言模型 AlignGPT。在預訓練階段，我們為不同圖文對分配不同級別的對齊能力，而不是將所有圖文對等同看待。然後，在指令調整階段，我們自適應地結合這些不同級別的對齊能力，以滿足不同指令的動態對齊需求。廣泛的實驗結果顯示，我們的模型在 12 個基準上達到競爭性的性能。 | 對齊，多模態模型 |
| 2024年5月23日 | [HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models](https://arxiv.org/pdf/2405.14831) | 為了在充滿挑戰和變化多端的自然環境中生存，哺乳動物的大腦進化出儲存大量世界知識並不斷整合新信息的能力，同時避免災難性的遺忘。儘管大型語言模型（LLMs）在檢索增強生成（RAG）中取得了令人印象深刻的成就，但它們在預訓練後仍難以高效地整合大量新經驗。本研究提出了一種新型檢索框架 HippoRAG，受人類長期記憶的海馬索引理論啟發，旨在實現更深入和高效的知識整合。HippoRAG 協調 LLMs、知識圖譜和個性化 PageRank 算法，以模擬新皮質和海馬在記憶中的不同角色。我們將 HippoRAG 與現有的 RAG 方法在多跳問答上進行比較，顯示我們的方法顯著超越最先進的方法，最多達 20%。HippoRAG 的單步檢索性能與 IRCoT 相當或更好，同時成本降低 10-30 倍，速度提高 6-13 倍，並將 HippoRAG 整合到 IRCoT 中帶來了進一步的顯著提升。最後，我們展示了我們的方法能夠解決現有方法無法觸及的新場景。 | RAG 優化 |
| 2024年5月21日 | [OmniGlue: Generalizable Feature Matching with Foundation Model Guidance](https://arxiv.org/pdf/2405.12979) | 圖像匹配領域不斷湧現新的可學習特徵匹配技術，在傳統基準上取得了不斷提高的性能。然而，我們的研究顯示，儘管這些技術有所進步，它們在真實世界應用中的潛力因其對新圖像域的有限泛化能力而受限。本文介紹了 OmniGlue，第一個以泛化為核心原則設計的可學習圖像匹配器。OmniGlue 利用視覺基礎模型的廣泛知識指導特徵匹配過程，提高對訓練時未見域的泛化能力。此外，我們提出了一種新的關鍵點位置引導注意力機制，將空間和外觀信息解耦，從而增強匹配描述符。我們在包括場景級、物體中心和航拍圖像的 7 個數據集上進行了全面實驗。OmniGlue 的新組件在未見域上的相對增益達到 20.9%，相較於直接可比較的參考模型，同時在超越最近的 LightGlue 方法 9.5%。代碼和模型可在 https://hwjiang1510.github.io/OmniGlue 獲得。 | 多模態模型 |
| 2024年5月20日 | [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2405.12130) | 低秩適配（LoRA）是一種流行的參數高效微調（PEFT）方法，用於大型語言模型（LLMs）。本文分析了低秩更新在 LoRA 中的影響。我們的發現表明，低秩更新機制可能限制了 LLMs 有效學習和記憶新知識的能力。受此觀察啟發，我們提出了一種新方法，稱為 MoRA，使用方形矩陣實現高秩更新，同時保持相同的可訓練參數數量。為了實現這一點，我們引入了相應的非參數操作員，以減少方形矩陣的輸入維度並增加輸出維度。此外，這些操作員確保權重可以合併回 LLMs，使我們的方法可以像 LoRA 一樣部署。我們在指令微調、數學推理、持續預訓練、記憶和預訓練等五個任務上對我們的方法進行了全面評估。我們的方法在記憶密集型任務上超越了 LoRA，並在其他任務上達到可比性能。我們的代碼將在 https://github.com/kongds/MoRA 獲得。 | PEFT 方法，微調 |
| 2024年5月19日 | [Your Transformer is Secretly Linear](https://arxiv.org/pdf/2405.12250) | 本文揭示了一個新穎的線性特性，僅存在於 Transformer 解碼器中，包括 GPT、LLaMA、OPT、BLOOM 等模型。我們分析了序列層之間的嵌入轉換，發現了一個接近完美的線性關係（Procrustes 相似度得分為 0.99）。然而，當去除殘差部分時，線性度會下降，這是由於 Transformer 層的輸出範數持續較低。我們的實驗顯示，去除或線性近似 Transformer 中一些最線性的塊，不會顯著影響損失或模型性能。此外，在我們的小型模型預訓練實驗中，我們引入了一種基於余弦相似度的正則化，旨在減少層線性度。這種正則化在 Tiny Stories 和 SuperGLUE 等基準上的性能指標有所提高，並成功減少了模型的線性度。這項研究挑戰了對 Transformer 架構的既有理解，表明其運行可能比先前假設的更線性。 | Transformer 分析 |
| 2024年5月18日 | [Towards Modular LLMs by Building and Reusing a Library of LoRAs](https://arxiv.org/pdf/2405.11157) | 越來越多的參數高效適配（PEFT）基礎大型語言模型（LLM）的需求，促使我們研究是否可以重用這些訓練過的適配器以提高新任務的性能。我們研究了如何在多任務數據下構建一個適配器庫，並設計了技術來實現零樣本和監督任務泛化。我們對現有構建此適配器庫的方法進行了基準測試，並介紹了一種基於模型的聚類方法（MBC），該方法根據適配器參數的相似性對任務進行分組，間接優化多任務數據集的轉移。我們提出了一種新穎的零樣本路由機制 Arrow，該機制能夠根據新輸入動態選擇最相關的適配器，而無需重新訓練。我們在 Phi-2 和 Mistral 等多個 LLMs 上進行了廣泛的任務測試，驗證了基於 MBC 的適配器和 Arrow 路由在新任務泛化方面的優越性能。我們邁向創建模塊化、可適應的 LLMs，可以匹敵或超越傳統聯合訓練。 | PEFT 方法，微調，工具包 |
| 2024年5月16日 | [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818) | 我們介紹 Chameleon，一系列基於早期融合的混合模態模型，能夠在任意序列中理解和生成圖像和文本。我們從一開始就概述了一種穩定的訓練方法、一個對齊配方以及針對早期融合、基於標記的混合模態設置的架構參數化。這些模型在廣泛的任務中進行了評估，包括視覺問答、圖像標註、文本生成、圖像生成和長篇混合模態生成。Chameleon 展現了廣泛和通用的能力，包括在圖像標註任務中達到最先進的性能，在純文本任務中超越 Llama-2，並與 Mixtral 8x7B 和 Gemini-Pro 等模型競爭，在單一模型中實現非平凡的圖像生成。它還在一項新的長篇混合模態生成評估中，根據人類判斷匹配或超越了包括 Gemini Pro 和 GPT-4V 在內的更大模型的性能，其中提示或輸出包含混合的圖像和文本序列。Chameleon 標誌著在統一建模全多模態文件方面的一個重大進步。 | 多模態模型, 基礎模型 |
| 2024年5月16日 | [Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/pdf/2405.09798) | 大型語言模型以在少樣本上下文學習 (ICL) 中的有效性而著稱。最近的多模態基礎模型的進展使得上下文窗口前所未有的長，為探索其在更多示例中執行 ICL 的能力提供了機會。在這項工作中，我們評估了多模態基礎模型從少樣本到多樣本 ICL 的性能。我們在涵蓋多個領域（自然影像、醫學影像、遙感和分子影像）和任務（多類、多標籤和細粒度分類）的10個數據集上對 GPT-4o 和 Gemini 1.5 Pro 進行了基準測試。我們觀察到，多樣本 ICL，包括多達近2,000個多模態示例，比少樣本（<100示例）ICL 在所有數據集上都顯著提高。此外，Gemini 1.5 Pro 的性能在許多數據集上以對數線性方式持續提高，直至測試的最大示例數。鑑於多樣本 ICL 所需的長提示所帶來的高推理成本，我們還探討了在單次 API 調用中批量處理多個查詢的影響。我們顯示，在多個數據集上，將最多50個查詢批量處理在零樣本和多樣本 ICL 中可以提高性能，同時顯著降低每個查詢的成本和延遲。最後，我們測量了模型的 ICL 數據效率，即模型從更多示例中學習的速度。我們發現，儘管 GPT-4o 和 Gemini 1.5 Pro 在數據集上達到了相似的零樣本性能，但 Gemini 1.5 Pro 在大多數數據集上表現出更高的 ICL 數據效率。我們的結果表明，多樣本 ICL 可以使用戶有效地將多模態基礎模型適應新應用和領域。我們的代碼庫可在 https://github.com/stanfordmlgroup/ManyICL 上公開獲取。 | ICL, 多模態模型 |
| 2024年5月15日 | [LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673) | 低秩適應 (LoRA) 是大型語言模型中廣泛使用的參數高效微調方法。LoRA 通過僅訓練選定權重矩陣的低秩擾動來節省記憶體。在這項工作中，我們比較了 LoRA 和完整微調在兩個目標領域（編程和數學）中的性能。我們考慮了指令微調（約100K提示-響應對）和持續預訓練（約10B非結構化標記）數據體制。我們的結果表明，在大多數設置中，LoRA 的性能遠不及完整微調。儘管如此，LoRA 表現出一種理想的正則化形式：它更好地保持了基礎模型在目標領域之外的任務上的性能。我們顯示，LoRA 提供的正則化比常見技術（如權重衰減和隨機失活）更強；它還有助於保持更多樣化的生成。我們顯示，完整微調學習的擾動的秩比典型 LoRA 配置高出10-100倍，這可能解釋了一些報告的差距。我們最後提出了使用 LoRA 進行微調的最佳實踐。 | PEFT 方法, 微調 |
| 2024年5月14日 | [Understanding the performance gap between online and offline alignment algorithms](https://arxiv.org/pdf/2405.08448) | 從人類反饋中進行強化學習（RLHF）是大型語言模型對齊的標準框架。然而，離線對齊算法的日益流行挑戰了 RLHF 中在策略抽樣的必要性。在獎勵過度優化的背景下，我們從一組初始實驗開始，展示了在線方法相對於離線方法的明顯優勢。這促使我們通過一系列精心設計的實驗消融來調查性能差異的原因。我們實證表明，離線數據覆蓋和數據品質等假設本身無法令人信服地解釋性能差異。我們還發現，儘管離線算法訓練的策略擅長對生成對進行分類，但在生成方面表現較差；同時，通過在線算法訓練的策略在生成方面表現出色，但在對生成對進行分類方面表現較差。這暗示了區別性和生成性能力之間的獨特相互作用，這受到了抽樣過程的極大影響。最後，我們觀察到性能差異在對比和非對比損失函數中持續存在，並且似乎不能簡單通過擴大策略網絡來解決。總之，我們的研究揭示了策略抽樣在 AI 對齊中的關鍵角色，並暗示了離線對齊算法的一些基本挑戰。 | 對齊 |
| 2024年5月13日 | [RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/pdf/2405.07863) | 我們在這份技術報告中介紹了在線迭代人類反饋強化學習（RLHF）的工作流程，這在最近的大型語言模型（LLM）文獻中被廣泛報導為遠遠優於其離線對應方法。然而，現有的開源 RLHF 項目仍主要限於離線學習設置。在這份技術報告中，我們旨在填補這一空白，並提供一個詳細且易於複製的在線迭代 RLHF 食譜。特別是，由於在線人類反饋對於資源有限的開源社區通常是不可行的，我們首先使用多樣化的開源數據集構建偏好模型，並使用構建的代理偏好模型來近似人類反饋。然後，我們討論了在線迭代 RLHF 背後的理論見解和算法原則，並提供了詳細的實踐實現。我們訓練的 LLM，SFR-Iterative-DPO-LLaMA-3-8B-R，在 LLM 聊天機器人基準測試（包括 AlpacaEval-2、Arena-Hard 和 MT-Bench）以及其他學術基準測試（如 HumanEval 和 TruthfulQA）中取得了令人印象深刻的性能。我們已證明，通過完全開源的數據集，監督微調（SFT）和迭代 RLHF 可以獲得最先進的性能。此外，我們已將我們的模型、精選數據集和全面的逐步代碼指南公開。詳情請參考 https://github.com/RLHFlow/RLHF-Reward-Modeling 和 https://github.com/RLHFlow/Online-RLHF。 | 偏好優化, RLHF |
| 2024年5月2日 | [PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/pdf/2405.01535) | 專有語言模型（如 GPT-4）通常用於評估各種語言模型的響應質量。然而，包括透明性、可控性和經濟性在內的擔憂強烈推動了專門從事評估的開源語言模型的開發。另一方面，現有的開源評估語言模型存在關鍵缺點：1）它們發出的分數與人類評估者的分數顯著不同，2）它們缺乏進行直接評估和配對排名（最流行的兩種評估形式）的靈活性。此外，它們無法根據自定義評估標準進行評估，而是側重於有用性和無害性等一般屬性。為了解決這些問題，我們介紹了 Prometheus 2，一種比其前身更強大的評估語言模型，能夠緊密模仿人類和 GPT-4 的評估。此外，它能夠處理根據用戶定義的評估標準分組的直接評估和配對排名格式。在四個直接評估基準和四個配對排名基準上，PROMETHEUS 2 在所有測試的開源評估語言模型中，獲得了最高的相關性和與人類及專有語言模型評估者的一致性。我們的模型、代碼和數據全部公開。 | 評估, 代理 |
| 2024年5月2日 | [WILDCHAT: 1M CHATGPT INTERACTION LOGS IN THE WILD](https://arxiv.org/pdf/2405.01470) | 如今，像 GPT-4 和 ChatGPT 這樣的聊天機器人為數百萬用戶提供服務。儘管它們被廣泛使用，但仍缺乏展示這些工具在實際用戶群體中如何使用的公共數據集。為了填補這一空白，我們向在線用戶提供免費訪問 ChatGPT 的機會，以換取他們匿名同意收集他們的聊天記錄和請求標頭。由此，我們編譯了 WILDCHAT，一個包含100萬用戶與 ChatGPT 之間對話的語料庫，包含超過250萬個交互回合。我們將 WILDCHAT 與其他流行的用戶聊天機器人交互數據集進行比較，發現我們的數據集提供了最具多樣性的用戶提示，包含最多的語言數量，並呈現出研究人員研究潛在有害使用情況的最豐富的多樣性。除了帶時間戳的聊天記錄外，我們還豐富了數據集的各種人口統計數據，包括州、國家和雜湊 IP 地址，以及請求標頭。這一增強使得能夠更詳細地分析不同地區和時間維度的用戶行為。最後，由於它捕捉了廣泛的用例，我們展示了數據集在微調指令跟隨模型中的潛在用途。WILDCHAT 在 https://wildchat.allen.ai 以 AI2 ImpACT 許可證發布。 | 基準測試, 評估 |
| 2024年5月2日 | [STORYDIFFUSION: CONSISTENT SELF-ATTENTION FOR LONG-RANGE IMAGE AND VIDEO GENERATION](https://arxiv.org/pdf/2405.01434) | 對於最近基於擴散的生成模型來說，在一系列生成的圖像中保持一致的內容，尤其是包含主體和複雜細節的圖像，構成了重大挑戰。在這篇論文中，我們提出了一種新的自注意力計算方法，稱為一致性自注意力，顯著提高了生成圖像之間的一致性，並以零樣本的方式增強了流行的預訓練基於擴散的文本到圖像模型。為了將我們的方法擴展到長程視頻生成，我們進一步引入了一個新的語義空間時間運動預測模塊，稱為語義運動預測器。它被訓練來估計提供的兩張圖像之間在語義空間中的運動條件。該模塊將生成的圖像序列轉換為具有平滑過渡和一致主體的視頻，尤其在長視頻生成背景下，比僅基於潛在空間的模塊更穩定。通過合併這兩個新的組件，我們的框架稱為 StoryDiffusion，可以用一致的圖像或視頻描述文本故事，包含豐富的內容。我們希望提出的 StoryDiffusion 可以激發更多來自架構修改方面的研究。 | 多模態模型, 擴散 |
| 2024年5月2日 | [FLAME : Factuality-Aware Alignment for Large Language Models](https://arxiv.org/pdf/2405.01525) | 對齊是微調預訓練大型語言模型（LLM）以遵循自然語言指令並作為有用的 AI 助手的標準程序。我們觀察到，傳統的對齊過程無法提高 LLM 的事實準確性，並且經常導致更多錯誤事實（即幻覺）的生成。在這篇論文中，我們研究了如何使 LLM 對齊過程更加事實化，首先識別在對齊的兩個步驟（監督微調（SFT）和強化學習（RL））中導致幻覺的因素。特別是，我們發現，對 LLM 訓練新的知識或不熟悉的文本會促進幻覺。這使得 SFT 在訓練人類標記數據時不太事實，這些數據對 LLM 來說可能是新的。此外，標準 RL 中使用的獎勵函數也會促進幻覺，因為它引導 LLM 在多樣化的指令集上提供更有幫助的響應，通常偏好更長和更詳細的響應。基於這些觀察，我們提出了事實性對齊（FLAME），包括事實性 SFT 和通過直接偏好優化的事實性 RL。實驗表明，我們提出的事實性對齊引導 LLM 輸出更多事實性的響應，同時保持指令跟隨能力。 | 對齊, 事實性 |
| 2024年5月2日 | [NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment](https://arxiv.org/pdf/2405.01481) | 將大型語言模型（LLM）與人類價值和偏好對齊對於使其有用和安全至關重要。然而，構建有效的對齊工具可能具有挑戰性，特別是對於通常包含數十億或數百億參數的最大和最有能力的 LLM。我們創建了 NeMo-Aligner，一個可有效擴展到使用數百個 GPU 進行訓練的對齊工具包。NeMo-Aligner 提供了主要對齊範式的高度優化和可擴展實現，如從人類反饋中進行強化學習（RLHF）、直接偏好優化（DPO）、SteerLM 和自我遊戲微調（SPIN）。此外，我們的工具包支持在參數高效微調（PEFT）設置中運行大多數對齊技術。NeMo-Aligner 被設計為可擴展，允許以最小的努力支持其他對齊技術。它以 Apache 2.0 許可證開源，我們邀請社區貢獻 https://github.com/NVIDIA/NeMo-Aligner。 | 對齊, 工具包 |
| 2024年5月1日 | [Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3](https://arxiv.org/pdf/2405.00664) | 本研究針對最新的大型語言模型 Llama-3 進行了目標模型編輯分析。我們探索了流行的模型編輯技術 ROME、MEMIT 和 EMMET，它們旨在進行精確的層干預。通過對多達4096個編輯進行評估，我們確定了最有效的編輯層，涵蓋三種不同策略：順序編輯、批量編輯和我們稱之為順序-批量編輯的混合方法。我們的發現表明，增加編輯批量大小可能比按順序使用較小的編輯批量更顯著地降低模型性能。據此，我們認為順序模型編輯是擴展模型編輯方法的重要組成部分，未來的研究應該集中於結合批量和順序編輯的方法。這一觀察表明了當前模型編輯方法在推向更大編輯批量時的潛在限制，我們希望它能為未來優化批量大小和模型編輯性能的研究鋪平道路。 | 模型編輯 |
| 2024年5月1日 | [LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report](https://arxiv.org/pdf/2405.00732) | 低秩適應（LoRA）已成為參數高效微調（PEFT）大型語言模型（LLM）中最廣泛採用的方法之一。LoRA 減少了可訓練參數和記憶體使用量，同時實現了與完全微調相當的性能。我們旨在評估使用 LoRA 微調的 LLM 在實際應用中的可行性。首先，我們測量了在10個基礎模型和31個任務上，共310個模型的 LoRA 微調模型的質量。我們發現，4位元 LoRA 微調模型在平均上比基礎模型高出34分，比 GPT-4 高出10分。其次，我們研究了最有效的基礎模型進行微調，並評估了任務複雜性啟發法在預測微調結果中的相關性和預測能力。最後，我們評估了 LoRAX 的延遲和併發能力，這是一個開源的多 LoRA 推理服務器，能夠使用共享的基礎模型權重和動態適配器加載，在單個 GPU 上部署多個 LoRA 微調模型。LoRAX 支持的 LoRA Land 是一個網絡應用程序，在單個 NVIDIA A100 GPU（80GB 記憶體）上托管25個 LoRA 微調的 Mistral-7B LLM。LoRA Land 突顯了使用多個專門化 LLM 相對於單一通用 LLM 的質量和成本效益。 | PEFT 方法, 微調 |

