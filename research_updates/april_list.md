| 日期          | 標題                                                                                                                                              | 摘要                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 主題                          |
|:--------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------|
| 2024年4月30日 | [Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)                                                                          | 本文介紹了Octopus v4，一種利用功能性標籤整合多個針對特定任務優化的開源語言模型的新方法。Octopus v4擅長將用戶查詢定向到最合適的模型並重新構建查詢以達到最佳性能，基於之前的版本（v1、v2和v3）增強了選擇和參數理解。此外，它探討了使用圖作為多模型協調的多功能數據結構。Octopus v4模型及其功能已在GitHub上提供實驗。                                                                                                                                                                                                                                                                                                                                                                                                    | 基礎LLM                |
| 2024年4月30日 | [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)                                              | 本文提出了同時預測多個未來標籤的訓練方法，提升了樣本效率而不增加訓練時間。通過使用多個輸出頭預測n個標籤，該方法提升了代碼和自然語言模型的下游能力。特別對於較大的模型，它在生成基準（如編碼）上持續優於單標籤預測，在解決問題任務上有顯著增益。此外，使用多標籤預測訓練的模型在推理速度上提高了三倍，即使在大批量大小下也提供了額外的效率。                                                                                                                                                                                                                                                              | 新架構                |
| 2024年4月30日 | [Extending Llama-3's Context Ten-Fold Overnight](https://arxiv.org/pdf/2404.19553)                                                                | Llama-3-8B-Instruct模型的上下文長度通過有效的QLoRA微調從8K擴展到80K，只需要在一台8xA800 GPU機器上花費8小時。這種擴展顯著提升了模型在各種評估任務（如NIHS和主題檢索）上的性能，同時在短上下文任務上保持了熟練度。令人驚訝的是，該擴展僅使用了來自GPT-4的3.5K合成訓練樣本，展示了LLM延長上下文長度的潛力。團隊計劃公開所有相關資源，包括數據、模型、數據生成管道和訓練代碼。                                                                                                                                                                                                                                                                                                  | 上下文長度                  |
| 2024年4月29日 | [Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/pdf/2404.18796)                       | 本文提出了使用LLM評估小組（PoLL）來代替依賴單一大型模型（如GPT-4）進行質量評估的挑戰。PoLL方法由大量較小的模型組成，在三個不同設置和六個數據集上優於單一大型評審。它表現出更少的模型內偏見，成本比單一評審低七倍，提供了一種具有成本效益且更可靠的LLM評估方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 評估                      |
| 2024年4月28日 | [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/pdf/2404.16873)                                                     | 本文提出了一種名為AdvPrompter的新方法，用於生成可讀的對抗性提示，應對LLM的破解攻擊。與現有的基於優化的方法不同，AdvPrompter在生成對抗性提示時只需數秒，比較現有方法快800倍，且不需要訪問TargetLLM的梯度。該方法在生成高質量目標對抗性後綴和低秩微調AdvPrompter之間交替進行。實驗結果表明，在AdvBench數據集上的性能達到最先進水平，並能轉移到封閉源黑盒LLM API。                                                                                                                                                                                                                                                                                                                                  | 對抗攻擊，評估 |
| 2024年4月28日 | [Capabilities of Gemini Models in Medicine](https://arxiv.org/pdf/2404.18416)                                                                     | Med-Gemini是一種專門用於醫療任務的多模態模型，在多個基準上超越了GPT-4，在醫療文本摘要和問題解答方面達到最先進的水平。憑藉其先進的長上下文推理能力，它在從醫療記錄中檢索針對性的任務上優於現有方法。儘管有前景，但在真實世界醫療應用中部署前需要進一步評估。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 特定領域LLM            |
| 2024年4月25日 | [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/pdf/2404.16821)             | InternVL 1.5是一個開源的多模態大型語言模型（MLLM），縮小了開源和專有商業模型在多模態理解上的差距。它引入了三項改進：強大的視覺編碼器，支持高達4K分辨率的動態高分辨率圖像處理，以及高質量的雙語數據集。在基準測試中的表現證明了它在開源和專有模型之間的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 多模態LLM                 |
| 2024年4月25日 | [Make Your LLM Fully Utilize the Context](https://arxiv.org/pdf/2404.16811)                                                                       | 本文引入了信息密集（IN2）訓練來應對當前LLM面臨的中間丟失挑戰。利用合成的長上下文問答數據集，IN2訓練強調了在長上下文中的細粒度信息感知。將此方法應用於Mistral-7B後，產生了FILM-7B（FILl-in-the-Middle），該模型在32K上下文窗口中能穩健地檢索信息。FILM-7B在現實世界的長上下文任務中提升了性能，同時在短上下文任務上保持了可比的性能。                                                                                                                                                                                                                                                                                                                                                                         | 上下文長度                  |
| 2024年4月25日 | [SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension](https://arxiv.org/pdf/2404.16790)          | 這項工作介紹了SEED-Bench-2-Plus，一個專門為評估多模態大型語言模型（MLLM）的文字豐富視覺理解而設計的基準。包含2.3K多項選擇題，涵蓋圖表、地圖和網頁，旨在全面模擬現實世界的文字豐富場景。對34個著名的MLLM進行的評估突出了當前在文字豐富視覺理解方面的局限性，強調了在該領域進一步研究和改進的必要性。SEED-Bench-2-Plus作為現有MLLM基準的有價值補充，提供了有洞察力的觀察結果並激發未來在文字豐富視覺理解方面的發展。                                                                                                                                                                                                                                                                      | 多模態LLM                 |
| 2024年4月23日 | [AURORA -M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/pdf/2404.00399) | 本文介紹了AURORA -M，一個多語言開源語言模型，訓練了英語、芬蘭語、印地語、日語、越南語和代碼。它在總訓練標籤數上超過2萬億，並在經過人類審查的安全指令上進行微調，符合拜登-哈里斯行政命令關於安全、可靠和可信的人工智慧開發和使用的要求。AURORA -M在多項任務和語言上進行了嚴格的評估，展示了其在多語言環境中的強健性，尤其是在安全評估中表現出色，超越了替代方案。                                                                                                                                                                                                                                                                                                      | 特定領域LLM            |
| 2024年4月23日 | [Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)                                                                                 | 本文介紹了多頭專家混合（MH-MoE），用於解決稀疏專家混合（SMoE）中的低專家激活和缺乏細粒度分析能力的問題。MH-MoE採用多頭機制將標籤分割成子標籤，將它們分配給不同的專家進行並行處理，然後重新整合。這種方法增強了專家激活，加深了上下文理解，並減輕了過擬合。MH-MoE易於實現，並且可以與其他SMoE模型無縫集成，如英語聚焦的語言建模、多語言語言建模和遮罩多模態建模任務所示。                                                                                                                                                                                                                                                         | 新架構                |
| 2024年4月22日 | [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)                                 | 本文介紹了phi-3-mini，一個緊湊的38億參數語言模型，訓練了3.3萬億標籤，提供了類似於Mixtral 8x7B和GPT-3.5的大型模型的競爭性能。在基準測試如MMLU（69%）和MT-bench（8.38）中取得了顯著的成績，phi-3-mini專為在移動設備上部署而設計。其創新之處在於其數據集，這是一個phi-2的擴展版本，包括經過高度過濾的網絡數據和合成數據。此外，通過對phi-3-small和phi-3-medium模型進行參數擴展的初步結果表明，其性能得到了進一步提升。                                                                                                                                                                                                                                                                                                              | 基礎LLM                |
| 2024年4月22日 | [How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study](https://arxiv.org/pdf/2404.14047)                                              | 本文探討了Meta的LLaMA3 LLM在低位量化下的性能，這對於資源有限的情景至關重要。儘管在15T標籤上進行了令人印象深刻的預訓練，但LLaMA3模型在低位寬度量化時表現出顯著的降級。在1-8位寬度上對10種量化方法進行的評估顯示，在超低位寬度情景下存在顯著的性能差距，強調了未來發展需要縮小這一差距以實現實際應用。                                                                                                                                                                                                                                                                                                                                                                                                                  | 量化                    |
| 2024年4月22日 | [FlowMind: Automatic Workflow Generation with LLMs](https://arxiv.org/pdf/2404.13050)                                                             | 本文介紹了FlowMind，利用生成預訓練變換器（GPT）等LLM來自動化機器人流程自動化（RPA）中的工作流程生成，克服了處理臨時任務的限制。FlowMind的通用提示配方以可靠的API為基礎，減輕了幻覺問題並確保數據機密性。它通過提供高層次的工作流程描述來簡化用戶交互，允許有效的檢查和反饋。在NCEN-QA數據集上的評估顯示了FlowMind的成功和其組件在增強用戶交互和工作流程生成方面的重要性。                                                                                                                                                                                                                                                                                           | LLM代理                      |
| 2024年4月22日 | [SnapKV: LLM Knows What You are Looking for Before Generation](https://arxiv.org/pdf/2404.14469)                                                  | 本文介紹了SnapKV，一種不需要微調的高效方法，用於在保持可比性能的情況下最大限度地減少LLM中的鍵值（KV）緩存大小。SnapKV利用從「觀察」窗口中識別的注意力頭特定提示特徵，自動壓縮KV緩存，通過選擇聚集的重要位置顯著減少了計算開銷和內存佔用。與基線模型相比，在處理長輸入序列時，生成速度提高了3.6倍，內存效率提高了8.2倍。                                                                                                                                                                                                                                                                                                                                                                      | 微調                     |
| 2024年4月21日 | [AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation](https://arxiv.org/pdf/2404.12753)                                 | 本文介紹了AutoCrawler，一種將LLM與爬蟲相結合以增強網絡自動化適應性的兩階段框架。針對傳統方法和獨立LLM代理的限制，AutoCrawler採用分層HTML結構，通過自上而下和回退操作進行漸進理解。綜合實驗驗證了該方法在處理多樣且變化的網絡環境中的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | LLM代理                      |
| 2024年4月21日 | [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://arxiv.org/pdf/2404.13013)                           | 本文介紹了Groma，一種配備了細粒度視覺感知能力的多模態大型語言模型（MLLM），能夠處理區域級任務，如字幕和視覺定位。Groma採用本地化視覺標籤機制，將圖像分解成感興趣區域，無縫地將區域標籤整合到用戶指令和模型響應中。通過策劃一個視覺定位指令數據集，Groma在標準引用和定位基準中表現優於僅依賴語言模型或外部模塊進行定位的MLLM。                                                                                                                                                                                                                                                                                                                         | 多模態LLM                 |
| 2024年4月18日 | [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](https://arxiv.org/pdf/2404.12253)                                   | 本文探討了在不依賴大量數據或微調的情況下增強LLM推理和計劃能力的挑戰。引入了AlphaLLM，將蒙特卡洛樹搜索（MCTS）與LLM相結合，建立了一個自我改進的循環。AlphaLLM包括提示合成、針對語言任務的高效MCTS方法和精確反饋的評論模型等組件。在數學推理任務上的實驗結果表明，AlphaLLM顯著提升了LLM性能，無需額外的標註，展示了其在複雜推理和計劃任務中的自我改進潛力。                                                                                                                                                                                                                                                                                                      | 指令調整              |
| 2024年4月18日 | [Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment](https://arxiv.org/pdf/2404.12318)                               | 本文探討了一種簡單的方法，使用從一個源語言的偏好數據中訓練的獎勵模型來對其他目標語言進行零樣本跨語言對齊。對摘要和開放式對話生成任務的評估表明，這種方法取得了成功，在超過70%的評估實例中，跨語言對齊模型受到人類的偏愛。令人驚訝的是，不同語言的獎勵模型有時比同語言的表現更好。研究還確定了在缺乏語言特定數據進行監督微調時的最佳對齊實踐。                                                                                                                                                                                                                                                                                                      | 指令調整              |
| 2024年4月18日 | [Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org/pdf/2404.12241)                                                    | 本文介紹了由MLCommons AI安全工作組開發的AI安全基準v0.5，用於評估針對聊天調整的語言模型的安全風險。它引入了一種有原則的方法，涵蓋單一使用案例和角色，以及13個危險類別的分類法和7個類別的測試。計劃在2024年發布的1.0版本旨在提供更深入的AI系統安全見解。儘管v0.5不應用於安全評估，但它提供了詳細的文檔和評估工具，包括評分系統和一個名為ModelBench的開放平台。                                                                                                                                                                                                                                                                                                                              | 基準，評估           |
| 2024年4月16日 | [Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744)                                                          | 本研究提出了一種新方法，使具有20億參數的設備端語言模型在精度和延遲上超過GPT-4，同時將上下文長度減少了95%。該方法解決了雲環境中大型語言模型的隱私和成本問題，實現了在智能手機、汽車、VR頭顯和個人電腦等邊緣設備上的部署。通過提升延遲和減少推理成本，該方法符合現實應用的性能要求，適合在各種生產環境中的邊緣設備上部署。                                                                                                                                                                                                                                                                       | 小型LLM                      |
| 2024年4月15日 | [Learn Your Reference Model for Real Good Alignment](https://arxiv.org/pdf/2404.09656)                                                            | 現有的對齊問題方法不穩定，促使研究人員開發了各種技術。在語言模型對齊中，來自人類反饋的強化學習（RLHF）通過最小化政策之間的Kullback-Leibler散度來防止過擬合。直接偏好優化（DPO）旨在消除獎勵模型，但面臨限制。我們提出了信任區域DPO（TR-DPO），在訓練期間更新參考策略，在Anthropic HH和TLDR數據集上比DPO高出最多19%，在多個參數上提高了模型質量。                                                                                                                                                                                                                                                                                                                                                           | 提示工程              |
| 2024年4月15日 | [Compression Represents Intelligence Linearly](https://arxiv.org/pdf/2404.09937)                                                                  | 本文探討了LLM的壓縮和智能之間的關係，發現LLM壓縮外部文本語料庫的能力幾乎線性地與其智能相關（以基準分數衡量）。結果提供了實證證據，支持壓縮效率反映更高智能的觀點。此外，壓縮效率是與模型能力相關的可靠評估指標，並提供了開源數據集和管道供未來壓縮評估研究使用。                                                                                                                                                                                                                                                                                                                                                                        | 模型壓縮               |
| 2024年4月14日 | [Pre-training Small Base LMs with Fewer Tokens](https://arxiv.org/pdf/2404.08634)                                                                 | 本文介紹了Inheritune，一種簡單的方法，通過繼承變壓器塊並使用原始預訓練數據的一小部分來構建較小的語言模型。他們展示了其有效性，構建了一個使用僅10億標籤的15億參數語言模型，達到了與使用顯著更多數據訓練的公開可用模型相當的性能。此外，他們展示了使用較小模型的層從較大模型中獲取的情況下，較小的LLM可以在等效數據量上匹配其較大對應物的性能。廣泛的實驗驗證了Inheritune在不同設置中的有效性，代碼已在GitHub上公開。                                                                                                                                                                                                       | 小型LLM                      |
| 2024年4月14日 | [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies](https://arxiv.org/pdf/2404.08197)                  | 本文探討了在有限計算預算下縮小對比語言-圖像預訓練（CLIP）的數據、架構和訓練策略。強調了高質量數據的重要性，並建議對於較小數據集使用較小的ViT模型，對於較大數據集使用較大的ViT模型，計算固定。此外，還比較了四種訓練策略，發現CLIP+數據增強在使用一半數據的情況下達到了與CLIP相當的結果，為CLIP的訓練和部署提供了實用見解。                                                                                                                                                                                                                                                                                                                                                                                                      | 視覺模型                   |
| 2024年4月12日 | [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801)                              | Megalodon通過引入一種神經架構，解決了變壓器的二次複雜性和弱長度外推問題，用於高效的序列建模，具有無限上下文長度。它繼承了Mega的架構，並加入了增強功能，如複雜的指數移動平均（CEMA）、時間步驟正規化層、正規化注意力機制和預正規化雙跳殘差配置。在與Llama2的直接比較中，Megalodon展示了比具有70億參數和2萬億訓練標籤的變壓器更好的效率，達到了1.70的訓練損失，介於Llama2-7B（1.75）和13B（1.67）之間。                                                                                                                                                                                                                                                            | 上下文長度                  |
| 2024年4月11日 | [RULER: What's the Real Context Size of Your Long-Context Language Models?](https://arxiv.org/pdf/2404.06654)                                     | 常用於評估長上下文語言模型的針在草堆中（NIAH）測試，評估了從長干擾文本中檢索信息的能力。然而，它僅衡量了長上下文理解的表面形式。為了提供更全面的評估，引入了一個新的合成基準RULER。RULER擴展了NIAH測試，包含多種類型和數量的針，並引入了多跳追踪和聚合等新任務類別，以測試超越上下文搜索的行為。對10個長上下文語言模型的13個代表性任務進行的評估顯示，隨著上下文長度的增加，性能大幅下降，儘管在NIAH測試中幾乎達到了完美的準確性。只有四個模型能在32K標籤的長度上保持令人滿意的性能。RULER開源以鼓勵對長上下文語言模型的全面評估。 | 上下文長度                  |
| 2024年4月11日 | [Social Skill Training with Large Language Models](https://arxiv.org/pdf/2404.04204)                                                              | 本文探討了進入專業領域的社交技能障礙，並提出了一種利用大型語言模型進行社交技能訓練的解決方案，通過通用框架實現。提出的AI Partner和AI Mentor框架將體驗式學習與現實練習和量身定制的反饋相結合。該工作呼籲跨學科創新，以應對對勞動力發展和社會平等的更廣泛影響。                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 對齊                       |
| 2024年4月11日 | [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/pdf/2404.07965)                                                                       | 傳統語言模型預訓練方法對所有標籤一視同仁，但我們的研究表明並非所有標籤都同樣重要。我們引入了Rho-1，一種選擇性訓練與期望分佈對齊的標籤的新模型，在數學任務中提高了30%的少樣本準確性。經過微調後，Rho-1在MATH數據集上達到最先進的結果，與現有模型相比顯著減少了預訓練標籤數量。此外，將Rho-1預訓練於一般標籤上，提升了多樣任務的性能，增加了語言模型預訓練的效率和效果。                                                                                                                                                                                                                                                                                           | 新架構                |
| 2024年4月11日 | [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://arxiv.org/pdf/2404.07839)                                   | 本文介紹了RecurrentGemma，一個使用Google的新Griffin架構的開放語言模型。Griffin將線性回歸與局部注意力相結合，在語言建模上取得了優異的性能。它具有固定大小的狀態，減少了內存使用並實現了長序列的高效推理。我們提供了一個具有20億非嵌入參數的預訓練模型，以及一個指令調整變體。這兩個模型在使用較少標籤進行訓練的情況下，達到了與Gemma-2B相當的性能。                                                                                                                                                                                                                                                                                                                                                                                                                             | 新架構                |
| 2024年4月11日 | [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](https://arxiv.org/pdf/2404.07973)                        | 本文介紹了Ferret-v2，這是Ferret的升級版本，克服了LLM在區域理解中的局限性。Ferret-v2引入了三個關鍵改進：（1）任何分辨率的定位和引用，用於改進高分辨率圖像處理。（2）使用DINOv2編碼器的多粒度視覺編碼，以更好地捕捉多樣的視覺上下文。（3）三階段訓練範式，包括高分辨率密集對齊，在引用和定位任務上顯著優於Ferret和其他最先進的方法。                                                                                                                                                                                                                                                                                                                                                               | 基準，評估           |
| 2024年4月10日 | [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](https://arxiv.org/pdf/2404.07413)                                                         | 本文介紹了JetMoE-8B，一個具有成本效益且高性能的大型語言模型，使用最少的資源進行訓練。其高效架構顯著減少了與先前模型相比的計算量，同時其透明性鼓勵合作和可訪問的LLM開發的進步。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 基礎LLM                |
| 2024年4月9日  | [Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models](https://arxiv.org/pdf/2404.06209)                    | 本文探討了LLM如何處理表格數據，重點是記憶和過擬合問題。研究發現LLM記住了受歡迎的表格數據集，在這些數據集上表現更好，表明存在過擬合。研究還強調了在未經微調的情況下，LLM的上下文統計學習能力有限，強調了評估LLM在預訓練期間是否見過評估數據集的重要性。本文介紹了tabmemcheck Python包，用於測試數據集的暴露情況。                                                                                                                                                                                                                                                                                                                                                                                                                          | 特定領域LLM            |
| 2024年4月9日  | [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143)                        | 本文介紹了一種高效的方法，用於將基於變壓器的LLM擴展到無限長的輸入，具有有限的內存和計算。該方法的關鍵組件是一種稱為Infini-attention的新注意力技術。Infini-attention將壓縮記憶引入到原生注意力機制中，並在單個變壓器塊中構建了遮罩局部注意力和長期線性注意力機制。該方法在長上下文語言建模基準、1M序列長度密碼上下文塊檢索和500K長度書籍摘要任務中的1B和8B LLM上展示了其有效性。該方法引入了最小的有限內存參數，實現了LLM的快速流式推理。                                                                                                                                                                          | 上下文長度                  |
| 2024年4月8日  | [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/pdf/2404.05961)                                            | 大型解碼器語言模型（LLM）在NLP任務中表現出色，但在文本嵌入方面未得到充分利用。本研究介紹了LLM2Vec，一種將解碼器語言模型轉換為強大文本編碼器的方法，通過雙向注意力、遮罩標籤預測和對比學習。應用於1.3B到7B參數的LLM，LLM2Vec在單詞級任務上超越了僅使用編碼器的模型，並在大規模文本嵌入基準（MTEB）上達到了新的無監督最先進水平。與監督對比學習的整合進一步提升了性能，展示了從LLM中創建通用文本編碼器的潛力，無需昂貴的適應或合成數據。                                                                                                                                                                                                                                                         | 新架構                |
| 2024年4月7日  | [Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/pdf/2404.03683)                                                        | 本文介紹了搜索流（SoS）的概念，通過在語言中表示過程，教語言模型進行搜索。SoS使用倒計時遊戲進行演示，模型被訓練結合輸入數字和算術運算來達到目標數字。對SoS進行預訓練提高了25%的搜索準確性，進一步微調使模型能夠解決36%的先前未解決的問題。這種方法使語言模型能夠學習解決問題的策略，並有潛力發現新策略。                                                                                                                                                                                                                                                                                                                                                                                                   | 特定領域LLM            |
| 2024年4月4日  | [Long-context LLMs Struggle with Long In-context Learning](https://arxiv.org/pdf/2404.02060)                                                      | 本研究引入了一個專門的基準，LongICLBench，重點關注極限標籤分類領域的長上下文學習。該基準評估了13個長上下文LLM，數據集的輸入長度從2K到50K標籤不等，標籤範圍從28到174個類別。儘管長上下文LLM在演示長度較短的較輕鬆任務上表現相對較好，但在更困難的任務上表現不佳，在最具挑戰性的任務（具有174個標籤的Discovery）上接近零準確性。進一步分析揭示了當前LLM在處理和理解長上下文序列方面的能力缺口，表明需要改進未來LLM的長上下文理解和推理能力。                                                                                                                                                                  | 上下文長度                  |
| 2024年4月4日  | [ReFT: Representation Finetuning for Language Models](https://arxiv.org/pdf/2404.03592)                                                           | 本文介紹了表示微調（ReFT）方法，作為參數高效微調（PEFT）方法的替代方案，用於適應大型語言模型。ReFT方法操作於凍結的基礎模型上，並學習針對特定任務的干預隱藏表示，旨在編輯表示而不是修改權重。一個強大的ReFT實例稱為低秩線性子空間ReFT（LoReFT），在各種評估任務中實現了比先前PEFT高出10-50倍的參數效率。LoReFT在各種評估任務中展示了效率和性能的最佳平衡，並在 https://github.com/stanfordnlp/pyreft 上公開發布了通用的ReFT訓練庫。                                                                                                                                                                                    | 微調，PEFT               |
| 2024年4月4日  | [Training LLMs over Neurally Compressed Text](https://arxiv.org/pdf/2404.03626)                                                                   | 本文探討了使用神經文本壓縮器進行高壓縮文本的LLM訓練。儘管標準子詞分詞器僅能對文本進行小比例壓縮，但神經文本壓縮器可以實現更高的壓縮率。直接在神經壓縮文本上訓練LLM的主要障礙是強壓縮往往會產生不透明的輸出，不利於學習。為了解決這一問題，本文提出了等信息窗口，一種將文本分段為壓縮到相同位長度的區塊的壓縮技術。該方法使得在神經壓縮文本上的有效學習成為可能，隨著規模增長而改進，在困惑度和推理速度基準上超越字節級基線。本文還提供了進一步改進高壓縮分詞器的建議。                                                                                                                       | 模型壓縮               |
| 2024年4月4日  | [CODE EDITOR BENCH: EVALUATING CODE EDITING CAPABILITY OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2404.03543)                                | 本文介紹了CodeEditorBench，一個用於嚴格評估LLM在代碼編輯任務中性能的評估框架，包括調試、翻譯、打磨和需求切換。CodeEditorBench強調了通過從各種來源策劃的多樣化編碼挑戰和場景，注重軟件開發的現實場景和實踐方面。對19個LLM的評估表明，封閉源模型（特別是Gemini-Ultra和GPT-4）在CodeEditorBench上優於開源模型，突出了基於問題類型和提示敏感性的模型性能差異。CodeEditorBench旨在通過提供一個強大的平台來促進LLM的進步，評估代碼編輯能力，並將釋放所有提示和數據集以使社區擴展數據集和基準測試新興LLM。                                                            | 評估                      |
| 2024年4月4日  | [GPT-4V Red-teamed under 11 Different Safety Policies](https://arxiv.org/pdf/2404.03411)                                                          | 本文提出了一個全面的破解評估數據集，包括針對11種安全策略的1445個有害問題。對11個不同的LLM和多模態大型語言模型（MLLM）進行了廣泛的紅隊實驗，包括最先進的專有模型和開源模型。結果顯示GPT4和GPT-4V在應對破解攻擊方面的穩健性優於開源模型。值得注意的是，在開源模型中，Llama2和Qwen-VL-Chat表現出更高的穩健性。與文本破解方法相比，視覺破解方法的可轉移性相對有限。                                                                                                                                                                                                                                                                                                   | 紅隊評測                     |
| 2024年4月4日  | [RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](https://arxiv.org/pdf/2404.03204)           | RALL-E提出了一種用於文本到語音（TTS）合成的強大語言建模方法，解決了LLM中的不穩定語調和高詞錯率（WER）問題。該方法使用思維鏈提示將任務分解為簡單步驟，預測輸入文本的語調特徵，並將其用作中間條件來預測語音標籤。此外，RALL-E使用預測的持續時間提示來引導自注意力權重，提升對應音素和語調特徵的關注度。客觀和主觀評估顯示，與基線方法相比，RALL-E在WER方面有顯著改進，展示了在減少錯誤率方面的有效性。                                                                                                                                                                | 提示工程              |
| 2024年4月4日  | [CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues](https://arxiv.org/pdf/2404.03820)                                     | 本文介紹了CANT TALKABOUT THIS數據集，旨在使語言模型在對話中保持話題相關性。它由帶有分散注意力回合的合成對話組成，以分散聊天機器人對預定話題的注意力。在該數據集上進行訓練提高了語言模型保持話題和在指令跟隨任務中的表現，包括安全對齊                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 對齊                       |
| 2024年4月3日  | [On the Scalability of Diffusion-based Text-to-Image Generation](https://arxiv.org/pdf/2404.02883)                                                | 本文通過對擾動消除骨幹和訓練集進行廣泛的消融研究，實證研究了基於擴散的文本到圖像（T2I）模型的可擴展性。該研究探索了各種訓練設置和訓練成本，以了解如何有效地擴展模型以在降低成本的同時提高性能。研究結果表明，增加變壓器塊比增加通道數更具參數效率，有助於改進文本和圖像的對齊。此外，訓練集的質量和多樣性對文本圖像對齊性能和學習效率有顯著影響。提供了縮放函數來預測基於模型大小、計算和數據集大小的文本圖像對齊性能。                                                                                                                                                              | 多模態LLM                 |
| 2024年4月2日  | [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://arxiv.org/pdf/2404.01258)                    | 本文介紹了一個新框架，使用詳細的視頻字幕作為代理，對大型多模態模型（LMM）進行與視頻內容對齊。該框架通過納入信息豐富的反饋，提升了視頻LMM在視頻問答（QA）任務中的性能，改進了生成回應的準確性。該方法利用直接偏好優化（DPO）引導LMM生成在多模態上下文中更準確、有幫助且無害的內容。                                                                                                                                                                                                                                                                                                                                                                                                                 | 多模態LLM                 |
| 2024年4月2日  | [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/pdf/2404.02078)                                                     | 本文介紹了EURUS，一套優化推理的大型語言模型。通過微調Mistral-7B和CodeLlama-70B，EURUS模型在涵蓋數學、代碼生成和邏輯推理問題的多樣基準上達到最先進的水平。在具有挑戰性的基準（如LeetCode和TheoremQA）上，EURUS超越了現有的開源模型，超出13.3%以上的邊際。EURUS的強大性能歸因於ULTRA INTERACT，一個專為複雜推理任務設計的大規模對齊數據集，以及從偏好學習技術中衍生出的新獎勵建模目標。                                                                                                                                                                                                                                                                                               | 特定領域LLM            |
| 2024年4月2日  | [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/pdf/2404.02258)                        | 本文介紹了一種用於動態分配計算的變壓器方法，優化模型深度各層的分配。通過限制參與每層計算的標籤數，該方法使用具有流動標籤身份的靜態計算圖，實現了高效的計算分配。使用該方法訓練的模型匹配基線性能，但每次前向傳遞所需的浮點運算數（FLOPs）更少，加快了訓練和抽樣速度。                                                                                                                                                                                                                                                                                                                                        | 新架構                |
| 2024年4月1日  | [LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model](https://arxiv.org/pdf/2404.01331)                          | 本文介紹了LLaVA-Gemma，一套使用LLaVA框架訓練的多模態基礎模型，使用了Gemma家族的LLM，特別是具有20億參數的Gemma模型。該研究評估了對三個設計特徵的影響：連接器的預訓練，使用更強大的圖像骨幹，增加語言骨幹的大小。儘管LLaVA-Gemma在各種評估中表現中等，但未能超越當前同等大小的最先進模型。本文釋放了LLaVA-Gemma模型的訓練配方、代碼和權重，以促進該領域的進一步研究。                                                                                                                                                                                                                                                                                            | 多模態LLM                 |

