# 介紹 MM LLMs

![應用 LLMs - 2024-05-03T122206.505.png](img/Applied_LLMs_-_2024-05-03T122206.505.png)

### **1. 多模態 LLMs 簡介**

大多數人對 OpenAI 的 Sora 在 2024 年 2 月首次亮相時感到非常震驚，因為它能無縫地生成逼真的影片。Sora 是多模態 LLM (MM-LLM) 的一個絕佳範例，使用文本來影響影片的生成，這是一個已經發展了數年的研究方向。特別是在過去的一年中，MM-LLM 取得了顯著的進展，為能夠處理和生成跨多種模態內容的新時代 AI 鋪平了道路。這些 MM-LLM 代表了傳統 LLM 的重大演變，因為它們整合了來自文本、圖像和音頻等各種來源的資訊，以增強其理解和生成能力。

值得注意的是，並非所有多模態系統都是 MLLMs。雖然一些模型結合了文本和圖像處理，但真正的 MLLMs 涵蓋了更廣泛的模態，並無縫整合它們以增強理解和生成能力。本質上，MM-LLMs 通過具有成本效益的訓練策略增強了現成的 LLMs，使它們能夠支持多模態輸入或輸出。通過利用 LLMs 固有的推理和決策能力，MM-LLMs 賦予了多種多模態任務的能力，涵蓋自然語言理解、計算機視覺和音頻處理。

另一個值得注意的 MLLM 範例是 OpenAI 的 GPT-4(Vision)，它結合了 GPT 系列的語言處理能力和圖像理解能力。使用 GPT-4(Vision)，模型可以生成基於文本的圖像描述、回答有關視覺內容的問題，甚至為圖像生成標題。同樣地，Google 的 Gemini 和 Microsoft 的 KOSMOS-1 是開創性的 MLLM，展示了在處理文本和圖像方面令人印象深刻的能力。

MLLMs 的應用範圍廣泛且多樣。MLLMs 可以分析文本輸入以及附帶的圖像或音訊，以獲得更深入的見解和上下文。例如，它們可以通過考慮文本內容和附帶的圖像來協助社交媒體帖子的情感分析。在電腦視覺中，MLLMs 可以通過結合文本描述或音訊提示來增強圖像識別任務，從而獲得更準確且上下文相關的結果。此外，在虛擬助手和聊天機器人等應用中，MLLMs 可以利用多模態輸入來提供更具吸引力和個性化的用戶互動。

超越這些範例，MLLMs 有潛力改進各種行業和領域，包括醫療保健、教育、娛樂和自主系統。通過無縫整合來自不同模態的資訊，MLLMs 可以使 AI 系統更好地理解和與世界互動，最終導致更智能和類人行為。

在本指南的以下部分中，我們將探討MLLMs的核心組件、訓練範式、最新進展、評估方法、挑戰和未來方向，揭示這一突破性技術的激動人心的可能性和影響。

## **2. 核心元件**

![Screenshot 2024-05-02 at 12.06.29 PM.png](img/Screenshot_2024-05-02_at_12.06.29_PM.png)

圖片來源: [https://arxiv.org/pdf/2401.13601](https://arxiv.org/pdf/2401.13601)

大多數 MM-LLMs 可以分為關鍵組件，每個組件由特定的設計選擇區分。在本指南中，我們將採用論文 "MM-LLMs: Recent Advances in Multimodal Large Language Models」 ([link](https://arxiv.org/pdf/2401.13601)) 中概述的組件框架。這些組件旨在無縫整合來自不同模態的資訊，如文本、圖像、影片、音頻等，使模型能夠理解和生成跨多個模態的內容。

**2.1 模態編碼器**

模態編碼器（ME）在MM-LLMs中扮演著關鍵角色，通過將來自各種模態的輸入編碼成相應的特徵表示。其功能類似於將來自不同模態的資訊翻譯成模型可以有效處理的通用格式。例如，ME處理圖像、影片、音頻和3D數據，將它們轉換為捕捉其基本特徵的特徵向量。這一步對於促進模型對多模態輸入的後續處理至關重要。不同模態的模態編碼器範例如[ViT](https://huggingface.co/docs/transformers/model_doc/vit)、[OpenCLIP](https://github.com/mlfoundations/open_clip)等。

**2.2 輸入投影儀**

當來自不同模態的輸入被編碼成特徵表示後，輸入投影器開始發揮作用。這個元件將其他模態的編碼特徵與文本特徵空間對齊，使模型能夠有效整合來自多個來源的資訊。通過將不同模態的特徵與文本特徵對齊，輸入投影器確保模型能夠生成連貫且上下文相關的輸出，這些輸出包含了輸入中所有模態的資訊。這可以使用線性投影器、多層感知器（MLP）、交叉注意力、[Q-Former](https://huggingface.co/docs/transformers/main/en/model_doc/blip-2)等來實現。

**2.3 LLM 主幹**

在 MM-LLMs 的核心是語言模型骨幹，它處理來自各種模態的表示，進行語義理解、推理和對輸入的決策。LLM 骨幹生成來自其他模態的文本輸出和信號標記，作為指導生成過程的指令。通過利用預訓練 LLM 的能力，MM-LLMs 繼承了零樣本泛化和少樣本學習等屬性，使其能夠生成多樣且上下文相關的多模態內容。MM-LLMs 中常用的 LLM 範例包括 [Flan-T5](https://huggingface.co/docs/transformers/en/model_doc/flan-t5)、[PaLM](https://ai.google/discover/palm2/)、[LLaMA](https://llama.meta.com/llama3/) 或任何文本生成 LLM。

**2.4 輸出投影機**

Output Projector 作為 LLM Backbone 和 Modality Generator 之間的橋樑，將來自 LLM Backbone 的信號標記表示映射到 Modality Generator 可理解的特徵。此元件確保生成的多模態內容與模型產生的文本表示對齊。通過最小化映射特徵與條件文本表示之間的距離，Output Projector 促進了連貫且語義一致的多模態輸出生成。這可以使用具有可學習解碼器特徵序列的 Tiny Transformer 或 MLP 來實現。

**2.5 模態產生器**

最後，模態生成器負責根據對齊的文本表示產生不同模態的輸出。通過利用現成的[潛在擴散模型](https://huggingface.co/docs/diffusers/en/api/pipelines/latent_diffusion) (LDMs)，模態生成器合成與輸入文本和其他模態對齊的多模態內容。在訓練過程中，模態生成器利用真實內容學習生成連貫且上下文相關的多模態輸出。

### 2.1 任務範例:

讓我們了解這些元件如何在一個用例中協同工作，以生成多媒體內容的字幕，給定來自圖像和文本描述的輸入，如下圖所示:

![Screenshot 2024-05-02 at 12.21.46 PM.png](img/Screenshot_2024-05-02_at_12.21.46_PM.png)

以下是每個組件的運作方式:

1. **模態編碼器**:
    - 給定一個輸入圖像，模態編碼器使用預訓練的視覺編碼器（如 ViT）對圖像特徵進行編碼。
    - 同樣，文本描述使用 LLM 的語言編碼器進行編碼。
2. **輸入投影器**:
    - 輸入投影器將編碼的圖像特徵與文本特徵空間對齊。
    - 這種對齊確保圖像特徵與文本特徵相容，以便 LLM 骨幹進一步處理。
    - 它可能使用如交叉注意力的方法來有效對齊來自不同模態的特徵。
3. **LLM 骨幹**:
    - LLM 骨幹處理來自各種模態的對齊表示。
    - 它執行語義理解和推理，為給定的圖像和文本生成標題。
    - 此組件整合來自兩種模態的資訊，以生成連貫且上下文相關的標題。
4. **輸出投影器**:
    - 輸出投影器將生成的文本表示（標題）映射到模態生成器可理解的特徵。
    - 這種映射確保生成的標題被轉換為特徵，這些特徵可以引導模態生成器生成多媒體內容。
5. **模態生成器**:
    - 模態生成器接收映射的文本表示以及來自 LLM 骨幹的任何額外信號。
    - 基於這些輸入，模態生成器合成對應於生成標題的多媒體內容。
    - 合成的多媒體內容與文本描述對齊，並且可以包括圖像、影片或音頻，對應於給定的輸入。

## **3. 資料和訓練範式**

訓練 MM-LLMs 通常包括兩個步驟，類似於 LLMs:

### 3.1 預訓練 (MM-PT):

在 MM-LLM 的預訓練階段，模型會遇到包含各種模態對的大型數據集，例如圖像和文本、音頻和文本、影片和文本，或其他組合，具體取決於當前的任務。預訓練的目的是初始化模型的參數，並促進學習能夠捕捉不同模態及其相應文本描述之間有意義連接的表示。

在預訓練過程中，MM-LLM 獲得了從每種模態中提取特徵並將其合併以生成連貫表示的能力。

通常，使用三種主要類型的數據:

1. Gold Pairs: 此類別涉及精確匹配資料，其中的物件與相應的文字描述相關聯。例如，在圖像的情境中，這包括與其文字標題配對的圖像。
2. Interleaved Pairs: 第二種類型包括包含物件和文字的交錯文件。與通常包括簡潔且高度相關的文字的精確匹配資料不同，交錯資料往往包含較長且較多樣化的文字，與所伴隨的物件的平均相關性較低。這有助於增強系統的泛化能力和穩健性。在圖像的情境中，這類似於標題資料，但文字與周圍圖像的相關性較低。
3. Text Only: 此外，僅文字資料也被納入訓練過程中，以維持和增強基礎預訓練語言模型的語言理解能力。

以下表格來自[此](https://arxiv.org/pdf/2401.13601)論文，提供了流行訓練數據集及其大小的列表：

![螢幕截圖 2024-05-03 上午 10.55.00.png](img/Screenshot_2024-05-03_at_10.55.00_AM.png)

### 3.2 指令調整 (MM-IT):

在 MM-LLMs 的指令調整階段，模型會被微調以執行特定任務。我們以視覺問答（VQA）這樣的任務為例。

透過提供明確的指示以及輸入資料。讓我們使用一個VQA範例來分解這是如何運作的:

1. **任務制定**: 最初，任務被制定，在這個例子中是VQA。模型預期回答關於圖像的問題。
2. **指令模板**: 使用各種指令模板為模型提供如何處理輸入資料的指導。這些模板可以指定圖像和問題應如何呈現給模型。例如:
    - "<Image>{Question}" 問題的簡短答案是;
    - "<Image>" 檢查圖像並用簡短的答案回應以下問題: "{Question}。答案:"; 等等。
3. **在特定數據集上微調**: 使用根據選擇的指令模板結構化的任務特定數據集對MM-LLM進行微調。這些數據集可以是單輪QA或多輪對話，取決於任務的需求。在此階段，模型使用與預訓練相同的目標進行最佳化，但重點放在當前的任務上。
4. **通過人類反饋的強化學習（RLHF）**: 微調後，使用RLHF進一步提升模型的性能。RLHF涉及對模型的回應提供反饋，可以是手動或自動的。這些反饋，例如自然語言反饋（NLF），用於引導模型生成更好的回應。強化學習算法被用來有效地將不可微分的NLF整合到訓練過程中。模型被訓練生成基於所提供反饋的回應。

因此，MM-LLMs 的指令調整階段涉及在任務特定的數據集上對模型進行微調，同時提供明確的指令和反饋來指導學習過程，最終提高模型執行所需任務的能力。在上述範例中，我們研究了一個特定任務: 視覺問答。

## **4. 最先進的 MM-LLMs**

![Screenshot 2024-05-02 at 12.06.06 PM.png](img/Screenshot_2024-05-02_at_12.06.06_PM.png)

圖片來源: [https://arxiv.org/pdf/2401.13601](https://arxiv.org/pdf/2401.13601)

上述圖片列出了一些流行的 SoTA MM-LLMs。從高層次來看，不同的 MM-LLMs 在幾個關鍵方面有所不同:

1. **支援的模態**: MM-LLMs 可能支援各種模態，例如文字、圖片、音訊、影片等。有些模型專注於特定模態，而其他模型則提供對多種模態的更廣泛支援。
2. **架構設計**: MM-LLMs 採用不同的架構設計來整合和處理多模態輸入。這包括模態融合的方式變化、跨模態應用注意機制的方式，以及整體網路結構。
3. **資源效率**: MM-LLMs 在計算需求、模型大小和記憶體佔用方面的效率各異。有些模型優先考慮在受限平台上的資源效率，而其他模型則優先考慮性能。
4. **特定任務能力**: MM-LLMs 可能針對特定任務或領域進行調整，例如視覺問答、圖片標註、對話生成或跨模態檢索。每個模型的設計和訓練目標通常針對預期任務進行最佳化。
5. **遷移學習和微調**: 模型在遷移學習和微調的方法上可能有所不同。有些模型利用預訓練的語言或視覺模型並對其進行微調以適應多模態任務，而其他模型則從頭開始在多模態數據上進行訓練。
6. **基準性能**: 在基準數據集和任務上的性能也可能因 MM-LLMs 而異。模型可能在某些領域表現出色，或在處理特定類型的數據或模態方面表現出優勢。

為了選擇適合特定使用案例的 MM-LLM，請考慮以下因素：

- **任務需求**: 確認 MM-LLM 將被用於的特定任務或應用。
- **模態**: 確定資料中涉及的模態（例如，文字、圖像、音訊），並選擇支持這些模態的模型。
- **效能指標**: 評估模型在相關指標上的效能，例如準確性、F1 分數或在相關基準上的 BLEU 分數。
- **資源限制**: 考慮可用於部署的計算資源，因為某些模型可能比其他模型更具資源效率。

## **5. 評估 MM-LLMs**

MM-LLMs 可以使用各種指標和方法來評估其在不同任務和數據集上的表現。以下是評估 MM-LLMs 的維度:

1. **特定任務的指標**: 根據MM-LLM設計的任務，可以使用特定的評估指標。例如:
    - **視覺問答（VQA）**: 可以使用準確率或F1分數等指標來評估模型回答圖像問題的能力。一些基準包括OKVQA、IconVQA、GQA等。一些基準可以在[這裡](https://paperswithcode.com/task/visual-question-answering)找到。
    - **圖像描述**: 可以使用BLEU、METEOR或CIDEr等指標來評估生成的描述與人類撰寫的參考描述相比的品質。一些圖像描述的基準可以在[這裡](https://paperswithcode.com/task/image-captioning)找到。
    - **語音識別**: 常用的指標包括詞錯誤率（WER）或字元錯誤率（CER），用來評估模型生成的轉錄的準確性。
    - **跨模態檢索**: 可以使用平均精度（MAP）或Recall@K等評估指標來衡量模型在不同模態間檢索相關內容的性能。
2. **人工評估**: 人類評審可以通過主觀評估來評估MM-LLM生成的輸出的品質。這可以包括根據相關性、一致性和整體品質對生成的描述或回應進行排名，與人類撰寫的對比。
3. **零樣本或少樣本學習**: MM-LLM也可以通過其在最少或無特定任務訓練數據下執行任務的能力來進行評估。這涉及測試模型在未見過的任務或領域上的性能，從而提供其泛化能力的見解。
4. **對抗性評估**: 可以使用對抗性例子或壓力測試來評估MM-LLM在面對輸入擾動或對抗性攻擊時的穩健性。
5. **下游任務性能**: MM-LLM通常會在下游任務上進行性能評估，例如圖像分類、文本生成或情感分析，其中可以利用多模態表示來提高性能。
6. **遷移學習性能**: 評估也可以針對MM-LLM的表示在其他任務或領域上的遷移能力，這表明模型在跨模態學習有用且可泛化的表示的能力。

## **6. 挑戰與未來方向**

MM-LLMs 代表了一個快速增長的領域，具有廣泛的研究進展和實際應用潛力。

以下是一些值得探索的方向:

1. **更強大的模型**:
    - **擴展模態**: MM-LLM 可以通過支持除目前的圖像、影片、音頻、3D 和文本以外的其他模態來增強。包括網頁、熱圖和圖表/表格等模態可以增加多功能性。
    - **LLM 多樣化**: 結合各種類型和大小的 LLM，為從業者提供選擇最適合特定需求的模型的靈活性。
    - **改進 MM 指令調整 (IT) 數據集品質**: 增加 MM IT 數據集中的指令多樣性可以提高 MM-LLM 對用戶命令的理解和執行。
    - **加強 MM 生成能力**: 雖然許多 MM-LLM 專注於 MM 理解，但通過檢索為基礎的方法改進 MM 生成能力，有望提高整體性能。
2. **更具挑戰性的基準**:
    - 開發能夠充分挑戰 MM-LLM 的基準至關重要，因為現有數據集可能無法充分測試其能力。構建具有更多模態和統一評估標準的大規模基準可以幫助解決這一問題。
    - 引入如 GOAT-Bench、MathVista、MMMU 和 BenchLMM 等基準，評估 MM-LLM 在各種任務和模態下的能力。
3. **移動/輕量化部署**:
    - 在資源受限的平台（如移動和 IoT 設備）上部署 MM-LLM 需要輕量化實現。這方面的研究，如 MobileVLM 和類似研究，旨在以最小的資源使用實現高效計算和推論。
4. **與領域知識的整合**:
    - 將領域特定的知識和本體整合到 MM-LLM 中，可以增強其在專業領域的理解和推理能力，從而產生更準確和上下文相關的輸出。
5. **具身智能**:
    - 具身智能的進步使 MM-LLM 能夠模仿人類對環境的感知和互動。像 PaLM-E 和 EmbodiedGPT 這樣的工作展示了將 MM-LLM 與機器人整合的進展，但需要進一步探索以增強自主性。
6. **多語言和跨語言能力**:
    - 增強 MM-LLM 支持多種語言並促進跨語言理解和生成，可以擴大其在不同語言環境中的應用。
7. **隱私和安全**:
    - 開發技術以確保在 MM-LLM 中對多模態數據進行隱私保護和安全處理至關重要，特別是在醫療和金融等敏感領域。
8. **可解釋性和可解釋性**:
    - 研究改進 MM-LLM 決策的可解釋性和可解釋性的方法，可以增強其應用中的信任和透明度，特別是在關鍵決策過程中。
9. **持續學習**:

- 持續學習(CL)對於MM-LLMs來說至關重要，因為它可以有效利用新興數據，同時避免重新訓練的高成本。這一領域的研究，包括持續PT和IT，解決了災難性遺忘和負向前向傳遞等挑戰。

1. **減輕幻覺**:
    - 幻覺是指MM-LLMs在沒有視覺線索的情況下生成不存在物件的描述，這是一個重大挑戰。減輕幻覺的策略包括利用自我反饋作為視覺線索以及改進訓練方法以提高輸出可靠性。

## 參考資料:

1. MM-LLMs: 多模態大型語言模型的最新進展 ([link](https://arxiv.org/pdf/2401.13601))
2. MM1: 多模態LLM預訓練的方法、分析與見解 ([link](https://arxiv.org/pdf/2403.09611))
3. https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models
4. 多模態大型語言模型調查 ([link](https://arxiv.org/pdf/2306.13549))。

