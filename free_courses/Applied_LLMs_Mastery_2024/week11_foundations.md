# [第11週] LLM 基礎

## ETMI5: 用五分鐘解釋給我聽

在我們課程的第一週，我們探討了兩種類型的機器學習模型之間的區別：生成模型（LLMs 是其中的一部分）和判別模型。生成模型擅長從數據中學習並創造新事物。本週，我們將通過查看用於語言處理的神經網絡的歷史來了解 LLMs 是如何開發的。我們從循環神經網絡（RNNs）的基礎開始，然後轉向更高級的架構，如序列到序列模型、注意力機制和 transformers。我們還將回顧一些使用 transformers 的早期語言模型，如 BERT 和 GPT。最後，我們將討論我們今天使用的 LLMs 是如何建立在這些早期發展基礎上的。

## 生成模型 vs 判別模型

在第一週，我們簡要介紹了生成式 AI 的概念。需要注意的是，所有機器學習模型都屬於兩類之一：生成式或判別式。LLM 屬於生成式類別，這意味著它們學習文本特徵並將其應用於各種應用中。雖然我們不會深入探討數學上的複雜性，但理解生成式和判別式模型之間的區別對於大致了解 LLM 的運作方式是很重要的:

### **生成模型**

生成模型嘗試了解資料是如何生成的。它們學習資料中的模式和結構，以便能夠創建新的類似資料點。

例如，如果你有一個生成狗狗圖片的模型，它會學習構成狗狗的特徵和特性（如毛皮、耳朵和尾巴），然後它可以生成看起來真實的新狗狗圖片，即使這些圖片從未被見過。

### **判別模型**

區別模型，另一方面，則專注於根據接收到的輸入做出決策或預測。

使用相同的狗的圖像範例，判別模型會查看圖像並決定其中是否包含狗。它不會擔心資料是如何生成的；它只關心根據所給的輸入做出正確的決定。

因此，生成模型學習數據中的潛在模式以建立新樣本，而判別模型則專注於根據輸入數據做出決策或預測，而不必擔心數據是如何生成的。

**基本上，生成模型建立，而判別模型分類或預測。**

## 神經網路用於語言

近幾年來，神經網路已成為機器學習的重要組成部分。其中，一個高度依賴神經網路的顯著類別模型被稱為深度學習模型。最初用於文本生成的神經網路類型被稱為循環神經網路（RNN）。隨後出現了改進版本，如長短期記憶網路（LSTM）、雙向LSTM和門控循環單元（GRU）。現在，讓我們來探索RNN如何生成文本。

### 循環神經網路 (RNN)

遞歸神經網路 (RNNs) 是一種人工神經網路，旨在通過允許資訊在網路架構內的迴圈中持續存在來處理序列資料。傳統的神經網路缺乏隨時間保留資訊的能力，這在處理像文本、音頻或時間序列資料等序列資料時可能是一個主要的限制。

RNN 的基本原理是它們具有形成有向循環的連接，允許資訊從網路的一個步驟傳遞到下一個步驟。這意味著網路在特定時間步驟的輸出不僅取決於當前的輸入，還取決於先前的輸入和網路的內部狀態，這些狀態捕捉了較早時間步驟的資訊。

![Screenshot 2024-02-23 at 10.24.38 AM.png](img/Screenshot_2024-02-23_at_10.24.38_AM.png)

圖片來源: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

以下是 RNNs 如何運作的簡化說明:

1. **輸入處理**: 在每個時間步驟 $t$，RNN 接收一個輸入 $x_t$。這個輸入可以是序列中的單一元素（例如句子中的一個詞）或代表輸入資料某些方面的特徵向量。
2. **狀態更新**: 輸入 $x_t$ 與前一時間步驟的網路內部狀態 $h_{t-1}$ 結合，使用網路內的一組加權連接（參數）來產生新的狀態 $h_t$。這個更新過程允許網路保留來自先前時間步驟的資訊。
3. **輸出生成**: 當前狀態 $h_t$ 用於在當前時間步驟生成輸出 $y_t$。這個輸出可以用於各種任務，例如分類、預測或序列生成。
4. **循環連接**: RNN 的關鍵特徵是存在循環連接，這允許資訊隨時間在網路中流動。這些連接在網路內創造了一種記憶形式，使其能夠捕捉序列資料中的相依性和模式。

雖然 RNN 是處理序列資料的強大模型，但它們可能會遇到某些限制，例如在學習長距離相依性方面的困難以及訓練期間梯度消失/爆炸問題。為了解決這些問題，已開發出更先進的 RNN 變體，例如長短期記憶（LSTM）網絡和門控循環單元（GRU）。這些架構包含了更好處理長期相依性和減輕梯度相關問題的機制，從而在廣泛的序列資料任務上提高了性能。

### 長短期記憶 (LSTM)

LSTM 網路因此是 RNN 的增強版本，旨在更好地處理像文本這樣的數據序列，就像 RNN 一樣，但具有以下改進:

![2024-02-23 上午10.32.53 的螢幕截圖.png](img/Screenshot_2024-02-23_at_10.32.53_AM.png)

![Screenshot 2024-02-23 at 10.33.00 AM.png](img/Screenshot_2024-02-23_at_10.33.00_AM.png)

圖片來源: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

1. **記憶體單元**: LSTM 有一個特殊的記憶體單元，可以隨時間儲存資訊。
2. **門控機制**: LSTM 使用門來控制資訊進入和離開記憶體單元的流動:
    - 輸入門: 決定保留多少新資訊。
    - 忘記門: 決定忘記多少舊資訊。
    - 輸出門: 決定輸出多少當前單元狀態。
3. **梯度流動**: LSTM 幫助梯度在訓練期間更好地流動，這有助於從長序列資料中學習。
4. **學習長期依賴性**: LSTM 擅長記住序列中較早的重要資訊，使其在需要理解長距離上下文的任務中非常有用。

因此，LSTM 更擅長處理序列，因為它們能記住重要的資訊並忘記不需要的部分，這使得它們在語言處理等任務上比傳統的 RNN 更有效。

RNN 和 LSTM（及其變體）廣泛用於語言建模任務，其目標是預測單詞序列中的下一個單詞。它們可以學習語言的基礎結構並生成連貫的文本。然而，它們難以處理可變長度的輸入序列並生成可變長度的輸出序列，因為其固定大小的隱藏狀態限制了它們捕捉長距離依賴關係和隨時間維持上下文的能力。

### 序列到序列 (Seq2Seq) 模型

這就是序列到序列（Seq2Seq）模型的作用所在；它們通過使用編碼器-解碼器架構來工作，其中輸入序列由編碼器編碼成固定大小的表示（上下文向量），然後由解碼器解碼成輸出序列。這種架構允許 Seq2Seq 模型處理可變長度的序列，並在生成相應的輸出序列時有效地捕捉輸入序列的語義和結構。下面描述了一個簡單的 Seq2Seq 模型。Seq2Seq 中的每個單元仍然是 RNN 類型的架構。

我們不會在此深入探討其運作原理以保持簡潔，[這篇](https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/#:~:text=Sequence%20to%20Sequence%20(often%20abbreviated,Chatbots%2C%20Text%20Summarization%2C%20etc)文章對於有興趣的人來說是很好的閱讀材料。

![s2s_11.png](img/s2s_11.png)

圖片來源: [https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)

### Seq2Seq 模型 + 注意力

![Screenshot 2024-02-24 at 2.19.47 PM.png](img/Screenshot_2024-02-24_at_2.19.47_PM.png)

圖片來源: [https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)

傳統 Seq2Seq 模型的問題在於它們無法有效處理長輸入序列，特別是在生成可變長度的輸出序列時。在標準的 Seq2Seq 模型中，使用固定長度的上下文向量來總結整個輸入序列，這可能會導致資訊丟失，特別是對於長序列。此外，在生成輸出序列時，解碼器可能難以集中於輸入序列的相關部分，導致次優的翻譯或預測。

為了解決這些問題，引入了注意力機制。注意力機制允許 Seq2Seq 模型在解碼過程中動態地關注輸入序列的不同部分。

**注意力的運作方式如下:**

1. **編碼器表示**: 首先，輸入序列由編碼器處理。編碼器將輸入序列的每個單詞或元素轉換為隱藏狀態。這些隱藏狀態代表輸入序列的不同部分，並包含有關序列內容和結構的資訊。
2. **計算注意力權重**: 在解碼過程中，解碼器需要決定要關注輸入序列的哪些部分。為此，它計算注意力權重。這些權重表示每個編碼器隱藏狀態對當前解碼步驟的相關性或重要性。本質上，模型試圖確定輸入序列的哪些部分對生成下一個輸出標記最為相關。
3. **Softmax 正規化**: 計算注意力權重後，模型使用 softmax 函式對它們進行正規化。這確保了注意力權重的總和為一，有效地將它們轉變為概率分佈。通過這樣做，模型可以確保它在輸入序列的不同部分適當地分配其注意力。
4. **加權和**: 在計算和正規化注意力權重後，模型接著對編碼器隱藏狀態進行加權和。本質上，它根據注意力權重確定的重要性或相關性，結合來自輸入序列不同部分的資訊。這個加權和代表了來自輸入序列的「關注」資訊，重點關注對當前解碼步驟最相關的部分。
5. **將上下文與解碼器狀態結合**: 最後，從加權和獲得的上下文向量與解碼器的當前狀態結合。這個結合表示包含了來自輸入序列（通過上下文向量）和解碼器先前狀態的資訊。它作為生成當前解碼步驟輸出的基礎。
6. **對每個解碼步驟重複**: 步驟 2 到 5 對每個解碼步驟重複，直到生成序列結束標記或達到最大長度。在每個步驟中，注意力機制幫助模型決定在輸入序列中應關注的部分，使其能夠生成準確且上下文相關的輸出序列。

### 轉換器模型

Seq2Seq 模型的問題在於其計算效率低下以及無法有效捕捉長序列中的相依性。雖然注意力機制顯著提高了模型在解碼過程中專注於輸入序列相關部分的能力，但也因需要為每個解碼步驟計算注意力權重而引入了計算開銷。此外，如前所述，傳統的帶有注意力機制的 Seq2Seq 模型仍然依賴於 RNN 或 LSTM 網路，這些網路在捕捉長距離相依性方面存在局限性。

Transformer 模型被引入以解決這些限制並提高序列到序列任務的效率和效果。以下是 Transformer 模型如何通過注意力機制解決 Seq2Seq 模型問題的方法:

![transformer_11](img/transformer_11.png)

圖像來源: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)

1. **自注意力機制**: Transformer 模型引入了自注意力機制，而不僅僅依賴於編碼器和解碼器之間的注意力機制。這種機制允許輸入序列中的每個位置關注所有其他位置，同時捕捉整個輸入序列中的依賴關係。與傳統的帶有注意力的 Seq2Seq 模型相比，自注意力使模型能夠更有效地捕捉長距離依賴關係。
2. **平行化**: Transformer 模型依賴於自注意力層，這些層可以對輸入序列中的每個位置進行平行計算。與傳統的帶有遞歸層的 Seq2Seq 模型相比，這種平行化大大提高了模型的計算效率，後者需要順序處理序列。因此，Transformer 模型可以更快地處理序列，使其更適合處理長序列和大規模數據集。
3. **位置編碼**: 由於 Transformer 模型不使用遞歸層，因此缺乏關於輸入序列中元素順序的內在資訊。為了解決這個問題，位置編碼被添加到輸入嵌入中，以提供關於序列中每個元素位置的資訊。位置編碼允許模型根據位置區分元素，確保模型能夠有效地處理具有順序元素的序列。
4. **Transformer 架構**: Transformer 模型由編碼器-解碼器架構組成，類似於傳統的 Seq2Seq 模型。然而，它用自注意力層取代了遞歸層，這使得模型能夠更有效地捕捉長序列中的依賴關係。此外，Transformer 架構允許更大的靈活性和延展性，使其更容易在各種任務和數據集上進行訓練和部署。

總結來說，Transformer 模型通過引入自注意機制、平行處理、位置編碼和靈活的架構，解決了 Seq2Seq 模型的注意力限制。這些進步提高了模型捕捉長距離依賴、有效處理序列的能力，並在各種序列到序列任務中達到最先進的表現。

### 舊語言模型

雖然 LLMs 最近獲得了顯著的關注，特別是來自 OpenAI 的 GPT 模型，但重要的是要認識到這種架構的基礎是由早期的模型如 BERT、GPT（舊版本）和 T5 所奠定的，如下所述。

LLMs 如 BERT (Bidirectional Encoder Representations from Transformers)、GPT (Generative Pre-trained Transformer) 和 T5 (Text-To-Text Transfer Transformer) 基於 Transformer 模型（在前幾節中描述）的概念，使用以下步驟建構:

1. **預訓練和微調**: 這些模型利用預訓練和微調的方法。在預訓練期間，模型使用無監督學習目標在大規模語料庫上進行訓練，例如掩碼語言建模(BERT)、自回歸語言建模(GPT)或文本到文本預訓練(T5)。這個預訓練階段使模型能夠從大量文本數據中學習豐富的語言表示和一般知識。預訓練之後，模型可以在帶標籤的數據上進行微調，以適應其學習到的表示來執行各種NLP任務，例如文本分類、問答和機器翻譯。
2. **雙向上下文**: BERT通過利用掩碼語言建模目標引入了雙向上下文建模。BERT不是以從左到右或從右到左的方式處理文本，而是通過掩碼部分輸入標記並根據周圍上下文進行預測來考慮雙向上下文。這種雙向上下文建模使BERT能夠捕捉文本中更深層的語義關係和依賴性，從而在廣泛的NLP任務中提高性能。
3. **自回歸生成**: GPT模型利用自回歸生成，模型根據先前生成的標記來預測序列中的下一個標記。這種方法使GPT模型能夠通過考慮生成序列的整個歷史來生成連貫且上下文相關的文本。GPT模型在涉及生成自然語言的任務（例如文本生成、對話生成和摘要）中特別有效。
4. **文本到文本方法**: T5引入了一個統一的文本到文本框架，將所有NLP任務框定為文本到文本映射問題。這種方法統一了各種NLP任務，例如翻譯、分類、摘要和問答，簡化了訓練和部署過程。T5通過將每個任務的輸入和輸出都表示為文本字符串來實現這一點，使模型能夠學習一個可以應用於不同任務的單一映射函數。
5. **大規模訓練**: 這些模型在包含數十億標記的大規模數據集上進行訓練，利用大量計算資源和分佈式訓練技術。通過在大量數據和強大硬件上進行訓練，這些模型能夠捕捉豐富的語言模式和語義關係，從而在廣泛的NLP任務中顯著提高性能。

### 大型語言模型

最新的 Llama，如 Llama 和 ChatGPT，在幾個關鍵方面相較於早期的模型如 BERT 和 GPT 代表了顯著的進步:

1. **任務專精**: 雖然早期的 LLM 如 BERT 和 GPT 被設計用來執行廣泛的 NLP 任務，包括文本分類、語言生成和問答，新型模型如 Llama 和 ChatGPT 更具專業化。例如，Llama 專門針對多模態任務，如圖像標註和視覺問答，而 ChatGPT 則針對對話應用進行了最佳化，如對話生成和聊天機器人。
2. **多模態能力**: Llama 和其他近期的 LLM 整合了多模態能力，使其能夠處理和生成與其他模態（如圖像、音頻和影片）結合的文本。這使得 LLM 能夠執行需要理解和生成跨多個模態內容的任務，為圖像標註、影片摘要和多模態對話系統等應用開啟了新的可能性。
3. **效率提升**: 最近在 LLM 架構和訓練方法上的進展導致了效率的提升，使得像 Llama 和 ChatGPT 這樣的模型能夠以更少的參數和計算資源達到與其前輩相當的性能。這種效率的提升使得在現實世界應用中部署這些模型更加實際，並減少了與訓練大型模型相關的環境影響。
4. **微調和遷移學習**: 像 ChatGPT 這樣的 LLM 通常會在特定數據集或任務上進行微調，以進一步提高在目標領域的性能。通過在特定領域數據上進行微調，這些模型可以調整其預訓練知識以更好地適應特定應用的需求，從而提高性能和泛化能力。
5. **互動和動態回應**: ChatGPT 和類似的對話模型被設計用來在自然語言對話中生成互動和動態的回應。這些模型利用對話中前幾輪的上下文來生成更連貫和上下文相關的回應，使其在聊天機器人應用和對話系統中更適合人類般的互動。

## 閱讀/觀看這些資源 (選擇性)

1. 理解 LSTM 網路: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. 序列到序列（seq2seq）和注意力機制: [https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
3. 序列到序列模型: [https://www.youtube.com/watch?v=kklo05So99U](https://www.youtube.com/watch?v=kklo05So99U)
4. 深度學習中的注意力機制: 理解序列模型中的注意力機制**:** [https://theaisummer.com/attention/](https://theaisummer.com/attention/)
5. LLMs 入門:
    1.  [https://www.youtube.com/watch?v=zjkBMFhNj_g&t=1845s](https://www.youtube.com/watch?v=zjkBMFhNj_g&t=1845s)
    2. [https://www.youtube.com/watch?v=zizonToFXDs](https://www.youtube.com/watch?v=zizonToFXDs)
6. Transformers: [https://www.youtube.com/watch?v=wl3mbqOtlmM](https://www.youtube.com/watch?v=wl3mbqOtlmM)

## 閱讀這些論文（可選）

1. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
2. [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
3. [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)

