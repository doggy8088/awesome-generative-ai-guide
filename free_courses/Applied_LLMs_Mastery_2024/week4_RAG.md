# [Week 4] 檢索增強生成

## ETMI5: 用五分鐘解釋給我聽

在本週的內容中，我們將深入探索檢索增強生成（RAG），這是一種通過在回應生成過程中整合來自外部來源的即時、上下文相關資訊來增強大型語言模型能力的 AI 框架。它解決了 LLMs 的局限性，例如不一致性和缺乏特定領域知識，從而降低生成錯誤或幻覺回應的風險。

RAG 在三個主要階段運作: 讀取、檢索和合成。在讀取階段，文件被分割成較小的、可管理的塊，然後轉換成嵌入並存儲在索引中以便高效檢索。檢索階段涉及利用索引根據相似度指標檢索出前 k 個相關文件，當收到用戶查詢時。最後，在合成階段，LLM 利用檢索到的資訊以及其內部訓練數據來制定準確的回應用戶查詢。

我們將討論 RAG 的歷史，然後深入探討 RAG 的關鍵組成部分，包括攝取、檢索和合成，提供每個階段的過程和改進策略的詳細見解。我們還將討論與 RAG 相關的各種挑戰，例如資料攝取的複雜性、高效嵌入以及為泛化進行微調，並提出每個挑戰的解決方案。

## 什麼是 RAG？(回顧)

增強檢索生成（RAG）是一種 AI 框架，通過在生成過程中結合來自外部來源的最新且上下文相關的資訊，提高 LLMs 生成的回應品質。它解決了 LLMs 中的不一致性和缺乏特定領域知識的問題，減少了幻覺或錯誤回應的機會。RAG 包含兩個階段：檢索階段，搜索並檢索相關資訊；內容生成階段，LLM 根據檢索到的資訊及其內部訓練數據綜合生成答案。這種方法提高了準確性，允許來源驗證，並減少了持續模型重新訓練的需求。

![Screenshot 2024-01-09 at 9.48.57 PM.png](img/Screenshot_2024-01-09_at_9.48.57_PM.png)

圖片來源: [https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)

上述圖表概述了基本的 RAG 管線，由三個關鍵組成部分構成:

1. **資料攝取:**
    - 文件被分割成塊，並從這些塊生成嵌入，隨後存儲在索引中。
    - 塊對於在回應給定查詢時定位相關資訊至關重要，類似於標準檢索方法。
2. **檢索:**
    - 利用嵌入的索引，系統在接收到查詢時根據嵌入的相似性檢索出前 k 個文件。
3. **綜合:**
    - 檢查塊作為上下文資訊，LLM 利用這些知識來制定準確的回應。

💡與先前的領域適應方法不同，值得強調的是，RAG 完全不需要任何模型訓練。當提供特定領域資料時，它可以直接應用而無需訓練。

## 歷史

RAG, 或檢索增強生成, 首次出現在Meta的[這篇](https://arxiv.org/pdf/2005.11401.pdf)論文中。這個想法是為了應對在大型預訓練語言模型中觀察到的限制，這些模型在有效訪問和操作知識方面存在不足。

![Screenshot 2024-01-27 at 1.37.28 PM.png](img/Screenshot_2024-01-27_at_1.37.28_PM.png)

圖片來源: [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)

以下是作者如何介紹問題並提供解決方案的簡短摘要:

RAG 的誕生是因為儘管大型語言模型在記憶事實和執行特定任務方面表現良好，但在精確使用和操作這些知識時卻顯得吃力。這在需要大量知識的任務中變得明顯，其他專門模型在這些任務中表現優於它們。作者發現現有模型存在一些挑戰，例如解釋決策的困難以及跟上現實世界變化的困難。在 RAG 出現之前，混合模型（結合參數記憶和非參數記憶）已經顯示出有希望的結果。像 REALM 和 ORQA 這樣的範例將掩碼語言模型與檢索器結合，在這個方向上顯示出積極的成果。

然後，RAG 出現了，作為檢索增強生成的靈活微調方法，改變了遊戲規則。RAG 結合了預訓練的參數記憶體（如 seq2seq 模型）和來自 Wikipedia 的密集向量索引的非參數記憶體，通過預訓練的神經檢索器如 Dense Passage Retriever (DPR) 訪問。RAG 模型旨在通過微調將預訓練的參數記憶體生成模型與非參數記憶體結合，以增強其功能。RAG 中的 seq2seq 模型使用神經檢索器檢索到的潛在文件，建立了一個端到端訓練的模型。訓練涉及在任意 seq2seq 任務上進行微調，同時學習生成器和檢索器。然後使用 top-K 近似處理潛在文件，無論是每個輸出還是每個標記。

RAG 的主要意義在於擺脫過去提議向系統添加非參數記憶體的方法。相反，RAG 探索了一種新方法，其中參數和非參數記憶體元件都經過預訓練並充滿了大量知識。在實驗中，RAG 通過在開放域問答中取得頂尖結果並超越先前的模型在事實驗證和知識密集型生成方面證明了其價值。RAG 的另一個勝利在於展示了它可以適應，允許非參數記憶體被替換和更新，以保持模型的知識在變化的世界中保持新鮮。

## 關鍵元件

如前所述，RAG 的關鍵元素涉及攝取、檢索和綜合的過程。現在，讓我們更深入地探討這些組成部分。

### 資訊攝取

在 RAG 中，攝取過程是指在資料被模型用來生成回應之前的處理和準備。

![2024-01-28 下午1.23.33 的螢幕截圖.png](img/Screenshot_2024-01-28_at_1.23.33_PM.png)

這個過程涉及3個關鍵步驟:

1. **Chunking: B**將輸入文本分解為更小、更易管理的段落或塊。這可以基於大小、句子或文本中的其他自然分割。我們將在下一部分深入探討分塊策略。舉個例子，考慮一篇關於文藝復興的綜合文章。分塊過程涉及根據自然斷點（如段落或不同的歷史時期（例如，早期文藝復興，高文藝復興））將文章分解為可管理的段落。每個這樣的段落成為一個塊，使語言模型能夠進行集中分析。
2. **Embedding**: 將文本或塊轉換為向量格式，以計算機友好的方式捕捉基本特徵。這一步對於語言模型的高效處理至關重要。延續前面的例子- 一旦文章段落被識別出來，嵌入過程將每個塊的內容轉換為向量格式。例如，高文藝復興部分可以嵌入到一個向量中，該向量捕捉關鍵的藝術、文化和歷史方面。這種向量表示增強了模型理解和處理塊內細微資訊的能力。
3. **Indexing:** 將嵌入的數據組織成優化的結構格式，以便快速高效地檢索。這通常涉及為每個文件創建一個向量表示，並將這些向量存儲在可搜索的格式中，如向量數據庫或搜索引擎。在我們討論的例子中- 索引數據庫是通過組織這些歷史事件的向量表示來創建的。每個塊，現在表示為一個向量，被索引以便高效檢索。當用戶查詢文藝復興的特定方面時，索引使得能夠快速識別和檢索最相關的塊，提供上下文豐富的回應。

### 檢索

檢索元件涉及以下步驟:

![2024-01-28 下午1.33.40 的螢幕截圖.png](img/Screenshot_2024-01-28_at_1.33.40_PM.png)

1. **使用者查詢:** 使用者向 LLM 提出自然語言查詢。例如，假設我們已經完成了文藝復興文章的攝取過程，如上面的方法所述，然後使用者提出查詢，「告訴我關於文藝復興時期的資訊」。
2. **查詢轉換:** 查詢被發送到嵌入模型，該模型將自然語言查詢轉換為數字格式，建立嵌入或向量表示。嵌入模型與在攝取階段用於嵌入文章的模型相同。
3. **向量比較:** 查詢的數字向量與在先前階段建立的知識庫索引中的向量進行比較。這涉及測量查詢向量與索引中存儲的向量之間的相似性或距離度量（通常是餘弦相似性）。
4. **Top-K 檢索:** 系統然後從知識庫中檢索與查詢向量最相似的前 K 篇文件或段落。這一步涉及根據向量相似性選擇預定數量（K）的最相關文件。這些嵌入可能包含有關文藝復興不同方面的資訊。
5. **數據檢索:** 系統從知識庫中選擇的前 K 篇文件中檢索實際內容或數據。這些內容通常是人類可讀形式，代表與使用者查詢相關的資訊。

因此，在檢索階段結束時，LLM 可以訪問與使用者查詢最相關的知識庫片段的相關上下文。在此範例中，檢索過程確保使用者收到關於文藝復興的充分資訊，並利用存儲在知識庫中的歷史文件提供豐富的上下文資訊。

### 合成

合成階段與常規的 LLM 生成非常相似，只是現在 LLM 可以訪問來自知識庫的額外上下文。LLM 向用戶呈現最終答案，將其自身的語言生成與從知識庫檢索到的資訊結合起來。回應可能包括對特定文件或歷史來源的引用。

![2024年1月28日下午1.34.09的截圖.png](img/Screenshot_2024-01-28_at_1.34.09_PM.png)

## RAG 挑戰

儘管 RAG 似乎是一種將 LLM 與知識整合的非常直接的方法，但 RAG 仍然存在以下提到的開放研究和應用挑戰。

1. **資料攝取複雜性:** 處理攝取大量知識庫的複雜性涉及克服工程挑戰。例如，有效地並行化請求、管理重試機制和擴展基礎設施是關鍵考量。想像一下攝取大量多樣化的數據來源，如科學文章，並確保後續檢索和生成任務的高效處理。
2. **高效嵌入:** 確保大數據集的高效嵌入會面臨挑戰，如解決速率限制、實施穩健的重試邏輯和管理自託管模型。考慮一個場景，AI系統需要嵌入大量新聞文章，這需要處理變化數據的策略、同步機制和最佳化嵌入成本。
3. **向量資料庫考量:** 將數據存儲在向量資料庫中引入了諸如理解計算資源、監控、分片和解決潛在瓶頸等考量。想像一下維護一個多樣化文件範圍的向量資料庫所涉及的挑戰，每個文件的複雜性和重要性各不相同。
4. **微調和泛化:** 微調RAG模型以適應特定任務，同時確保在多樣化知識密集型NLP任務中的泛化是具有挑戰性的。例如，在問答任務中實現最佳性能可能需要不同於創意語言生成任務的微調方法，這需要仔細平衡。
5. **混合參數和非參數記憶:** 在像RAG這樣的模型中整合參數和非參數記憶組件會帶來與知識修訂、可解釋性和避免幻覺相關的挑戰。考慮確保語言模型將其預訓練知識與動態檢索資訊相結合的困難，避免不準確性並保持連貫性。
6. **知識更新機制:** 開發機制以隨著現實世界知識的演變來更新非參數記憶是至關重要的。想像一個場景，RAG模型需要適應如醫學等領域中不斷變化的資訊，這些領域中新的研究發現和治療方法不斷湧現，需要及時更新以提供準確的回應。

## 改善RAG元件(Ingestion)

### 1. **更好的分塊策略**

在增強RAG元件的攝取過程中，採用先進的分塊策略是有效處理文本資料所必需的。在一個簡單的RAG管線中，採用固定策略，即固定數量的單詞或字符形成一個單一的塊。

考慮到大型數據集的複雜性，最近正在使用以下策略:

1. **基於內容的分塊:** 使用詞性標註或句法解析等技術根據意義和句子結構分解文本。這樣可以保留文本的意義和連貫性。然而，這種分塊的一個考量是它需要額外的計算資源和算法複雜性。
2. **句子分塊:** 使用句子邊界識別或語音片段將文本分解為完整且語法正確的句子。保持文本的統一性和完整性，但可能會生成大小不一的分塊，缺乏均勻性。
3. **遞歸分塊:** 將文本分割成不同層次的分塊，創建一個層次化且靈活的結構。提供更大的細粒度和文本多樣性，但管理和索引這些分塊涉及更高的複雜性。

### **2. 更好的索引策略**

改進的索引允許更有效率地搜尋和檢索資訊。當資料塊被適當地索引時，可以更容易地快速找到和檢索特定的資訊。一些改進的策略包括:

1. **詳細索引:** 通過子部分（例如，句子）進行分塊，並根據其位置分配每個塊一個識別符，並根據內容分配一個特徵向量。提供具體的上下文和準確性，但需要更多的記憶體和處理時間。
2. **基於問題的索引:** 通過知識領域（例如，主題）進行分塊，並根據其類別分配每個塊一個識別符，並根據相關性分配一個特徵向量。直接與用戶請求對齊，提高效率，但可能導致資訊丟失和準確性降低。
3. **帶有塊摘要的最佳化索引:** 使用提取或壓縮技術為每個塊生成摘要。根據摘要分配一個識別符，並根據相似性分配一個特徵向量。提供更大的綜合性和多樣性，但在生成和比較摘要時需要更高的複雜性。

## 改善 RAG 元件 (檢索)

### 1. **假設性問題和HyDE:**

假設性問題的引入涉及為每個塊生成一個問題，將這些問題嵌入向量中，並對這個問題向量索引進行查詢搜索。由於查詢和假設性問題之間的語義相似度較高，這提高了搜索品質。相反，HyDE (Hypothetical Response Extraction) 涉及在給定查詢的情況下生成假設性回應，通過利用查詢及其假設性回應的向量表示來提高搜索品質。

![2024-01-28 下午2.00.23 的螢幕截圖.png](img/Screenshot_2024-01-28_at_2.00.23_PM.png)

圖片來源: [https://arxiv.org/pdf/2212.10496.pdf](https://arxiv.org/pdf/2212.10496.pdf)

### 2. **Context Enrichment:**

這裡的策略旨在透過語言模型結合周圍上下文進行推理，以獲得更好的搜尋品質，同時目標是檢索較小的區塊。可以探索兩個選項:

1.  句子視窗檢索: 將文件中的每個句子分別嵌入，以在查詢與上下文之間的餘弦距離搜索中達到高精度。在檢索到最相關的單一句子後，通過包括檢索到的句子之前和之後的指定數量的句子來擴展上下文視窗。然後將這個擴展的上下文發送給LLM，以根據提供的查詢進行推理。目標是增強LLM對檢索到的句子周圍上下文的理解，從而能夠提供更有見地的回應。

![RAG.png](img/RAG.png)

圖片來源: [https://medium.com/@shivansh.kaushik/advanced-text-retrieval-with-elasticsearch-llamaindex-sentence-window-retrieval-cb5ea720aa44](https://medium.com/@shivansh.kaushik/advanced-text-retrieval-with-elasticsearch-llamaindex-sentence-window-retrieval-cb5ea720aa44)

1. 自動合併檢索器: 在這種方法中，文件最初被拆分成較小的子塊，每個子塊都引用一個較大的父塊。在檢索過程中，首先獲取較小的塊。如果在檢索到的頂部塊中，有超過指定數量的塊鏈接到同一個父節點（較大的塊），則餵給LLM的上下文將被這個父節點取代。這個過程可以類比為自動將幾個檢索到的塊合併成一個較大的父塊，因此得名「自動合併檢索器」。該方法旨在捕捉粒度和上下文，從而為LLM提供更全面和連貫的回應。

![RAG_1.png](img/RAG_1.png)

圖片來源: [https://twitter.com/clusteredbytes](https://twitter.com/clusteredbytes)

### 3. **融合檢索或混合搜索:**

這種策略將傳統的基於關鍵字的搜尋方法與當代的語義搜尋技術結合起來。通過結合多種演算法，如 tf-idf（詞頻-逆文件頻率）或 BM25 以及基於向量的搜尋，RAG 系統可以利用語義相關性和關鍵字匹配的優點，從而產生更全面和包容的搜尋結果。

![RAG_2.png](img/RAG_2.png)

圖片來源: [https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5](https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)

### 4. **重新排序與過濾:**

檢索後的改進是通過篩選、重新排序或轉換來進行的。LlamaIndex 提供各種後處理器，允許根據相似度分數、關鍵字、Metadata 進行篩選，或使用像 LLMs 或句子轉換器交叉編碼器等模型進行重新排序。這一步在將檢索到的上下文呈現給 LLM 以生成答案之前進行。

![RAG_3.png](img/RAG_3.png)

圖片來源: [https://www.pinecone.io/learn/series/rag/rerankers/](https://www.pinecone.io/learn/series/rag/rerankers/)。

### 4. **查詢轉換和路由[[Source](https://blog.langchain.dev/deconstructing-rag/)]**

查詢轉換方法通過將複雜查詢分解為子問題（擴展）並通過重寫改進措辭不佳的查詢來增強檢索。動態查詢路由則優化了在不同來源中的數據檢索。以下是流行的方法

### **查詢轉換**

1. **查詢擴展***:* 查詢擴展將輸入分解為子問題，每個子問題都是一個更狹窄的檢索挑戰。例如，關於物理學的問題可以退一步變成一個關於用戶查詢背後物理原理的問題（和 LLM 生成的答案）。
2. **查詢重寫**: 解決用戶查詢措辭不當或表述不佳的問題，[Rewrite-Retrieve-Read](https://arxiv.org/pdf/2305.14283.pdf?ref=blog.langchain.dev) 方法涉及重新措辭問題以增強檢索效果。該方法在論文中有詳細解釋。
3. 查詢壓縮: 在用戶問題跟隨更廣泛的聊天對話的情況下，可能需要完整的對話上下文來回答問題。查詢壓縮用於將聊天歷史壓縮為最終的檢索問題。

### **查詢路由**

1. **動態查詢路由**: 資料存放位置的問題在 RAG 中至關重要，特別是在具有多樣化資料存儲的生產環境中。由 LLMs 支援的動態查詢路由能有效地將傳入的查詢指向適當的資料存儲。這種動態路由能適應不同的來源並優化檢索過程。

## 改善 RAG 元件 (產生)

最直接的 LLM 生成方法是將所有相關的上下文片段連接起來，超過預定的相關性閾值，並將它們與查詢一起在單個實例中呈現給 LLM。然而，還存在更先進的替代方案，需要多次呼叫 LLM 以迭代地增強檢索到的上下文，最終生成更精緻和改進的答案。以下是一些方法的說明。

### 1. **回應綜合方法:**

涉及 3 步驟

1. **迭代改進:** 將檢索到的上下文逐塊發送到語言模型以改進答案。
2. **摘要:** 摘要檢索到的上下文以適應提示並生成簡明的答案。
3. **多重答案與串聯:** 根據不同的上下文塊生成多個答案，然後串聯或摘要它們。

### 2. **編碼器和 LLM 微調:**

這種方法涉及在我們的RAG管道中微調LLM模型。

1. **編碼器微調:** 微調 Transformer 編碼器以獲得更好的嵌入品質和上下文檢索。
2. **排序器微調:** 使用交叉編碼器重新排序檢索結果，特別是在對基礎編碼器缺乏信任的情況下。
3. **RA-DIT 技術:** 使用像 RA-DIT 這樣的技術來微調 LLM 和檢索器在查詢、上下文和答案的三元組上。

## 閱讀/觀看這些資源 (選擇性)

1. 建構生產就緒的 RAG 應用程式: [https://www.youtube.com/watch?v=TRjq7t2Ms5I](https://www.youtube.com/watch?v=TRjq7t2Ms5I)
2. Amazon 關於 RAG 的文章- [https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)
3. Huggingface 的 RAG 工具- [https://huggingface.co/docs/transformers/model_doc/rag](https://huggingface.co/docs/transformers/model_doc/rag)
4. 12 個 RAG 的痛點及建議解決方案- [https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c)

## 閱讀這些論文（可選）

1. [檢索增強生成用於大型語言模型: 一項調查](https://arxiv.org/pdf/2312.10997.pdf)

2. [構建檢索增強生成系統時的七個失敗點](https://arxiv.org/abs/2401.05856)

