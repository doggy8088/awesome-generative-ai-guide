# [第10週] 新興研究趨勢

## ETMI5: 用五分鐘解釋給我聽

在我們課程的這一部分中，我們將深入研究圍繞 LLMs 的最新研究發展。首先，我們將檢視多模態大型語言模型 (MM-LLMs)，探討這一特定領域如何迅速發展。接著，我們的討論將延伸到流行的開放原始碼模型，重點關注它們的建構和貢獻。隨後，我們將討論具備從開始到完成自動執行任務能力的代理概念。此外，我們將了解特定領域模型在豐富各行各業專業知識中的角色，並仔細觀察如專家混合和 RWKV 等突破性架構，這些架構將提升 LLMs 的延展性和效率。

## 多模態 LLMs (MM-LLMs)

在過去的一年裡，多模態大型語言模型（MM-LLMs）取得了顯著的進展。具體而言，MM-LLMs 代表了語言模型領域的一個重要演變，因為它們在文本處理能力之外還結合了多模態組件。儘管在一般的多模態模型方面也取得了進展，但 MM-LLMs 經歷了特別大的改進，這主要歸功於過去一年 LLMs 的顯著增強，MM-LLMs 在很大程度上依賴於這些增強。

此外，採用具成本效益的訓練策略大大促進了MM-LLMs的發展。這些策略使這些模型能夠有效地管理跨多種模態的輸入和輸出。與傳統模型不同，MM-LLMs不僅保留了大型語言模型固有的出色推理和決策能力，還擴展了其實用性，以解決跨各種模態的多樣化任務。

要了解 MM-LLMs 如何運作，我們可以檢視一些常見的架構元件。大多數的 MM-LLMs 可以分為五個主要元件，如下圖所示。以下解釋的元件來自於論文 「[MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/pdf/2401.13601.pdf)」。讓我們詳細了解每個元件。

![Screenshot 2024-02-18 at 3.09.34 PM.png](img/Screenshot_2024-02-18_at_3.09.34_PM.png)

圖片來源: [https://arxiv.org/pdf/2401.13601.pdf](https://arxiv.org/pdf/2401.13601.pdf)

**1. Modality Encoder:** Modality Encoder (ME) 在編碼來自不同模態的輸入 $I_X$ 以提取相應特徵 $F_X$ 中扮演關鍵角色。針對不同模態，有各種預訓練編碼器選項，包括視覺、音頻和3D輸入。對於視覺輸入，常用的選項包括 NFNet-F6、ViT、CLIP ViT 和 Eva-CLIP ViT。同樣地，對於音頻輸入，使用 CFormer、HuBERT、BEATs 和 Whisper 等框架。點雲輸入使用 ULIP-2 並以 PointBERT 為骨幹進行編碼。一些 MM-LLMs 利用 ImageBind，這是一個涵蓋多種模態的統一編碼器，包括圖像、影片、文本、音頻和熱圖。

**2. 輸入投影儀:** 輸入投影儀 $Θ_(X→T)$ 將其他模態的編碼特徵 $F_X$ 與文本特徵空間 $T$ 對齊。這種對齊對於將多模態資訊有效整合到 LLM Backbone 中至關重要。輸入投影儀可以通過各種方法實現，例如線性投影儀、多層感知器（MLPs）、交叉注意力、Q-Former 或 P-Former，每種方法都有其獨特的跨模態對齊特徵的方法。

**3. LLM Backbone:** LLM Backbone 作為 MM-LLMs 的核心代理，繼承了 LLMs 的顯著特性，如零樣本泛化、少樣本上下文學習 (ICL)、思維鏈 (CoT) 和指令遵循。Backbone 處理來自各種模態的表示，參與語義理解、推理和對輸入的決策。此外，一些 MM-LLMs 採用參數高效微調 (PEFT) 方法，如 Prefix-tuning、Adapter 或 LoRA，以最小化額外可訓練參數的數量。

**4. 輸出投影器:** 輸出投影器 $Θ_(T→X)$ 將信號標記表示 $S_X$ 從 LLM Backbone 映射到 Modality Generator $MG_X$ 可理解的特徵 $H_X$。此投影促進了多模態內容的生成。輸出投影器通常使用 Tiny Transformer 或 MLP 實現，其最佳化重點在於最小化映射特徵 $H_X$ 和 $MG_X$ 的條件文本表示之間的距離。

**5. 模態生成器:** 模態生成器 $MG_X$ 負責生成不同模態的輸出，例如圖像、影片或音訊。通常，現有的工作利用現成的潛在擴散模型（LDMs）來進行圖像、影片和音訊合成。在訓練過程中，真實內容被轉換為潛在特徵，然後通過去噪使用 LDMs 生成多模態內容，這些 LDMs 是基於從輸出投影器映射的特徵 $H_X$ 來調節的。

### 訓練

MM-LLMs 的訓練分為兩個主要階段: 多模態預訓練 (MM PT) 和多模態指令調整 (MM IT)。

**MM PT:**
在 MM PT 期間，MM-LLMs 被訓練來理解和生成來自不同類型數據的內容，如圖像、影片和文字。它們學會將這些不同種類的資訊對齊以協同工作。例如，它們學會將貓的圖片與「貓」這個詞聯繫起來，反之亦然。這個階段重點在於教模型處理不同類型的輸入和輸出。

**MM IT:**
在 MM IT 中，模型根據特定指令進行微調。這有助於模型適應新任務並在其上表現得更好。在 MM IT 中主要使用兩種方法:

- **監督式微調 (SFT):** 模型在包含指令的範例上進行訓練。例如，在問答任務中，每個問題都配對正確答案。這有助於模型學習遵循指令並生成適當的回應。
- **來自人類反饋的強化學習 (RLHF):** 模型會收到其回應的反饋，通常是人類生成的反饋。這些反饋幫助模型隨著時間的推移通過學習其錯誤來改進性能。

因此，MM-LLMs 被訓練來理解和生成來自多個資訊來源的內容，並且可以根據指示和反饋進行微調，以更好地執行特定任務。

下面的圖表總結了流行的 MM-LLM 及其各個組件使用的模型。

![Screenshot 2024-02-18 at 3.18.49 PM.png](img/Screenshot_2024-02-18_at_3.18.49_PM.png)

圖片來源: [https://arxiv.org/pdf/2401.13601.pdf](https://arxiv.org/pdf/2401.13601.pdf)

### 新興研究方向

一些 MM-LLMs 的潛在未來方向包括通過各種途徑擴展其能力：

1. **更強大的模型**:
    - 擴展 MM-LLMs 以適應當前如圖像、影片、音頻、3D 和文本以外的其他模態，例如網頁、熱圖和圖表/表格。
    - 結合各種類型和大小的 LLMs，為從業者提供靈活性，以選擇最適合其具體需求的模型。
    - 通過多樣化指令範圍來增強 MM IT 數據集，以改善 MM-LLMs 對用戶命令的理解和執行。
    - 探索整合基於檢索的方法，以補充 MM-LLMs 的生成過程，潛在地提升整體性能。
2. **更具挑戰性的基準**:
    - 開發更大規模的基準，包含更廣泛的模態，並使用統一的評估標準來充分挑戰 MM-LLMs 的能力。
    - 量身定制基準以評估 MM-LLMs 在實際應用中的熟練程度，例如評估其辨別和回應表情包中社會虐待細微方面的能力。
3. **移動/輕量化部署**:
    - 開發輕量化實現方案，以在資源受限的平台上部署 MM-LLMs，如低功耗移動和 IoT 設備，確保最佳性能。
4. **具身智能**:
    - 探索具身智能，以複製人類般的感知和與周圍環境的互動，使機器人能夠根據實時觀察自主實施擴展計劃。
    - 在現有進展如 PaLM-E 和 EmbodiedGPT 的基礎上，進一步增強基於 MM-LLM 的具身智能，以提高機器人的自主性。
5. **持續 IT**:
    - 開發方法使 MM-LLMs 能夠在保持先前學習任務的卓越性能的同時，不斷適應新的 MM 任務，解決如災難性遺忘和負向前傳等挑戰。
    - 建立基準並開發方法以克服 MM-LLMs 在持續 IT 中的挑戰，確保在不需要大量重新訓練成本的情況下高效適應新需求。

## 開放原始碼模型

最近開放原始碼LLM的發展在普及先進AI技術方面起到了關鍵作用。開放原始碼LLM相較於封閉原始碼模型具有多項優勢，增強了透明度、可定制性和協作性。它們允許對模型運作有更深入的理解，能夠根據具體需求進行修改，並通過社區貢獻鼓勵改進。它們還作為教育工具，支持多樣化的AI生態系統，防止壟斷。然而，計算需求和潛在的濫用等挑戰依然存在，但對於重視開放性和適應性的AI開發者來說，開放原始碼模型的好處通常超過這些問題。

一些熱門的開放原始碼 LLM 如下所列:

### **LLaMA by Meta**

- **LLaMA** (13B 參數) 由 Meta 於 2023 年 2 月發布，儘管參數較少，但在許多 NLP 基準上表現優於 GPT-3。**LLaMA-2** 是增強版本，擁有多 40% 的數據和雙倍的上下文長度，於 2023 年 7 月發布，並附帶專門用於對話 (**LLaMA 2-Chat**) 和程式碼產生器 (**LLaMA Code**) 的版本。

### **微風**

- 由巴黎的一家初創公司開發的 **Mistral 7B** 通過在英語和程式碼基準測試中超越所有現有的開放原始碼 LLMs（最多 13B 參數）設立了新的標杆。Mistral AI 隨後還發布了 **Mixtral 8x7B**，這是一個稀疏專家混合（SMoE）模型。這個模型標誌著傳統 AI 架構和訓練方法的轉變，旨在為開發者社群提供創新的工具，激發新的應用和技術。我們將在下一節中了解更多關於專家混合範式的內容。

### **開放語言模型 (OLMo)**

- **OLMo** 是 AI2 LLM 框架的一部分，旨在通過提供訓練數據、程式碼、模型和評估工具來鼓勵開放研究。它包括 **Dolma dataset**、全面的訓練和推論程式碼、四個 7B 規模變體的模型權重，以及 Catwalk 項目下的廣泛評估套件。

### **LLM360 計劃**

- **LLM360** 提出了一種完全開放原始碼的 LLM 開發方法，倡導發布訓練程式碼、資料、模型檢查點和中間結果。它發布了兩個 7B 參數的 LLM，**AMBER** 和 **CRYSTALCODER**，並提供了資源以確保 LLM 訓練的透明性和可重現性。

雖然 Llama 和 Mistral 只發布他們的模型，OLMo 和 LLM360 更進一步，提供檢查點、數據集等，確保他們的產品完全開放且可被複製。

## 代理

LLM 代理在最近幾個月中獲得了顯著的動力，並代表了 LLM 能力的未來和擴展。LLM 代理是一種 AI 系統，它在其核心使用大型語言模型來執行各種任務，不僅限於文本生成。這些任務包括進行對話、推理、完成各種任務，並根據提供的上下文和指示展示自主行為。LLM 代理通過複雜的提示工程運作，其中指示、上下文和權限被編碼以引導代理的行動和回應。

### **LLM代理的能力**

- **自主性**: LLM 代理可以根據其設計和接收到的提示，從反應性到主動性行為，具有不同程度的自主性。
- **任務完成**: 通過訪問外部知識庫、工具和推理能力，LLM 代理可以協助或獨立處理各種應用，從聊天機器人到複雜的工作流程自動化。
- **適應性**: 它們的語言建模能力使其能夠理解和遵循自然語言提示，使其具有多功能性並能夠定制其回應和行動。
- **高級技能**: 通過提示工程，LLM 代理可以配備高級分析、計劃和執行技能。它們可以在最少的人為干預下管理任務，依賴於其訪問和處理資訊的能力。
- **協作**: 它們通過回應互動提示並將反饋整合到其操作中，實現了人類與 AI 之間的無縫協作。

LLM 代理結合了 LLM 的核心語言處理能力與額外模組，如規劃、記憶體和工具使用，有效地成為指導一系列操作以完成任務或回應查詢的「大腦」。這種架構允許它們將複雜問題分解為可管理的部分，檢索和分析相關資訊，並根據需要生成全面的回應或視覺表示。

範例:

假設我們有興趣組織一個國際會議，討論可持續能源解決方案，旨在涵蓋可再生能源技術、能源生產中的可持續性實踐以及促進綠色能源的創新政策。這項任務涉及複雜的規劃和資訊收集，包括識別關鍵演講者、了解當前可持續能源的趨勢以及與利益相關者互動。

為了解決這個多方面的專案，可以使用一個 LLM 代理來：

1. **研究和總結**: 將任務分解為子任務，例如識別可持續能源的最新趨勢、尋找該領域的領先專家以及總結最近的研究成果。代理人將利用其訪問大量數字資源的能力來編寫綜合報告。
2. **演講者參與**: 起草個性化邀請函給潛在演講者，包含關於會議目標及其專業知識如何與會議目標一致的詳細資訊。代理人可以根據專家的個人資料和以前的工作生成這些通信。
3. **後勤規劃**: 為會議制定詳細計劃，包括活動時間表、後勤安排清單（場地、混合參與的虛擬平台設置等）以及參與者參與策略。代理人可以通過訪問活動規劃資源和最佳實踐的數據庫來概述這些計劃。
4. **利益相關者溝通**: 起草更新和新聞通訊給利益相關者，提供會議進展、議程亮點和確認的主要演講者的見解。代理人根據受眾（無論是贊助商、參與者還是公眾）量身定制每一篇通信。
5. **互動問答環節規劃**: 制定互動問答環節的框架，包括預先收集潛在參與者的問題、對其進行分類以及為演講者準備簡報文件。代理人可以通過分析註冊數據和提交的問題來促進這一過程。

在此情境中，LLM代理不僅協助執行複雜且耗時的任務，還確保規劃過程徹底，並根據最新的可持續能源發展進行資訊更新，針對會議的具體目標量身定制。通過利用外部資料庫、資料分析和視覺化工具及其內建的語言處理能力，LLM代理充當綜合助手，簡化了包含眾多移動部分的大型活動的組織工作。

LLM代理的框架可以通過各種視角來概念化，其中一個視角是由論文「[A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)」提供的，通過其獨特的組件。該架構由四個關鍵模組組成：Profiling Module、記憶體 Module、Planning Module 和 Action Module。這些模組中的每一個在使LLM代理能夠在各種情境中自主且有效地行動方面都起著至關重要的作用。

![Screenshot 2024-02-18 at 3.46.23 PM.png](img/Screenshot_2024-02-18_at_3.46.23_PM.png)

圖片來源: [https://arxiv.org/pdf/2308.11432.pdf](https://arxiv.org/pdf/2308.11432.pdf)

### **LLM 代理的組成部分**

1. **分析模組**

Profiling 模組負責定義代理的身份和角色。它包含年齡、性別、職業、性格特徵和社會關係等資訊，以塑造代理的行為。此模組使用各種方法來建立檔案，包括手工製作以進行精確控制、LLM 產生以實現延展性，以及數據集對齊以達到真實世界的準確性。代理的檔案顯著影響其互動、決策過程和執行任務的方式，使此模組成為代理設計的基礎。

**2. 記憶體模組**

記憶體模組儲存代理從其環境中感知到的資訊，並使用這些儲存的知識來指導未來的行動。它模仿人類的記憶過程，結構靈感來自感官、短期和長期記憶。此模組使代理能夠累積經驗，根據過去的互動進化，並以一致且有效的方式行動。它確保代理可以回憶過去的行為，從中學習，並隨著時間調整其策略。

**3. 規劃模組**

規劃模組賦予代理人將複雜任務分解為更簡單的子任務並單獨處理的能力，模仿人類的問題解決策略。它包括有反饋和無反饋的規劃，允許靈活適應變化的環境和需求。單一路徑推理和連鎖思維（CoT）等策略被用來引導代理人逐步實現其目標，使規劃過程對代理人的有效性和可靠性至關重要。

**4. 動作模組**

行動模組將代理的決策轉化為具體的結果，直接與環境互動。它考慮行動的目標、行動如何產生、可能行動的範圍（行動空間）以及這些行動的後果。此模組整合來自分析、記憶體和規劃模組的輸入，以執行與代理目標和能力相符的決策。這對於代理策略的實際應用至關重要，使其能夠在現實世界中產生具體的結果。

這些模組共同組成了一個全面的 LLM 代理架構框架，允許建立能夠承擔特定角色、感知並從環境中學習的代理，並能夠自主執行任務，其複雜性和靈活性達到模仿人類行為的程度。

### 未來研究方向

1. 大多數 LLM Agent 的研究僅限於基於文本的互動。擴展到多模態環境中，代理可以處理和生成各種格式的輸出，如圖像、音頻和影片，這引入了數據處理的複雜性，並要求代理解釋和回應更廣泛的感官輸入。
2. 幻覺，即模型生成事實上不正確的文本，在 LLM 代理系統中變得更加問題嚴重，因為有可能引發連鎖的錯誤資訊。開發檢測和減輕幻覺的策略涉及管理資訊流，以防止不準確的信息在網絡中傳播。
3. 雖然 LLM 代理從即時反饋中學習，但創建可靠的交互環境以進行可擴展的學習具有挑戰性。此外，目前的方法側重於單獨調整代理，未能充分利用多個代理之間協調互動所產生的集體智慧。
4. 擴展代理的數量（多代理系統）以適應某一用例會大幅增加計算需求，並在代理之間的協調和通信方面引入複雜性。開發高效的編排方法對於優化工作流程和確保多代理的有效合作至關重要。
5. 當前的基準可能無法充分捕捉對代理至關重要的突現行為，或涵蓋多樣的研究領域。開發全面的基準對於評估代理在各個領域（包括科學、經濟和醫療保健）中的能力至關重要。

## 特定領域 LLMs

雖然一般的 LLMs 多才多藝，並且在廣泛的任務中表現良好，但由於缺乏針對特定領域數據的訓練，它們在處理專門或利基任務時往往表現不佳。此外，執行這些通用模型可能成本高昂。在這些情況下，特定領域的 LLMs 成為更好的替代方案。它們的訓練專注於特定領域的數據，這提高了它們的準確性，並使它們對相關術語和概念有更深的理解。這種量身定制的方法不僅改善了它們在特定領域任務上的表現，還減少了生成不相關或不正確資訊的機會。

設計遵循各自領域的法規和道德標準，這些模型確保敏感資料的適當處理。由於掌握專業語言，它們還能與領域專家更有效地溝通。從經濟角度來看，特定領域的LLM通過消除大量手動調整的需求，提供了更高效的解決方案。此外，其專業知識庫使其能夠識別獨特的見解和模式，推動各自領域的創新。

一些受歡迎的特定領域 LLM 如下所示

### 流行的領域特定 LLMs

**臨床和生物醫學 LLMs**

- **BioBERT**: 一個在大規模生物醫學語料庫上預訓練的領域特定模型，旨在有效地挖掘生物醫學文本。
- **Hi-BEHRT**: 提供一個基於層次Transformer的結構，用於分析電子健康記錄中的擴展序列，展示了模型處理複雜醫療數據的能力。

**LLMs for Finance**

- **BloombergGPT**: 一個具有500億參數的金融專用模型，訓練於大量的金融數據，在金融任務中表現優異。
- **FinGPT**: 一個針對特定應用微調的金融模型，利用現有的LLM來增強金融數據的理解。

**特定程式碼的LLMs**

- **WizardCoder**: 透過複雜指令微調賦能程式碼 LLMs，展示對程式碼領域挑戰的適應性。
- **CodeT5**: 一個統一的預訓練模型，專注於程式碼中傳達的語義，強調開發者分配的識別符在理解程式設計任務中的重要性。

這些特定領域的 LLM 展示了 AI 在不同領域中的巨大潛力和適應性，從理解多語言內容和處理臨床數據到財務分析和程式碼產生器。通過專注於每個領域的獨特挑戰和數據類型，這些模型為 AI 應用中的創新、效率和準確性開闢了新的途徑。

### 領域特定LLM的未來趨勢

1. 特定領域的 LLMs 可能會演變成不僅能處理文本，還能處理圖像、音頻和其他數據類型，從而在各種格式中實現更全面的理解和互動能力。
2. 未來的模型可能會結合先進的互動學習技術，使它們能夠根據用戶反饋和新數據實時更新其知識庫，確保其輸出保持相關性和準確性。
3. 我們可能會看到更多系統中，特定領域的 LLMs 與其他 AI 技術（如決策算法和預測模型）協同工作，以提供整體解決方案（如我們在前一節中討論的 Agents）。
4. 隨著對 AI 社會影響的認識不斷提高，特定領域的 LLMs 的開發可能會更加強調倫理考量、公平性和透明度，特別是在醫療保健和金融等敏感領域。

## 新的 LLM 架構

### 專家混合

專家混合（MoEs）代表了變壓器模型領域內的一種複雜架構，重點在於提升模型的延展性和計算效率。以下是MoEs的概述及其重要性:

**定義和組成部分**

- **Transformer 中的 MoEs**: 在 transformer 模型中，MoEs 用稀疏的 MoE 層取代傳統的密集前饋網路 (FFN) 層。這些層包含多個「專家」，每個專家都是一個神經網路——通常是 FFN，但也可能是更複雜的結構，甚至是層次化的 MoEs。
- **專家**: 這些是處理特定數據部分的專門神經網路（通常是 FFN）。一個 MoE 層可能包含多個專家，例如 8 個，從而在同一模型層內提供多樣化的數據處理能力。
- **門網路/路由器**: 這是一個關鍵組件，根據學習到的參數將輸入標記導向適當的專家。路由器決定，例如，哪個專家最適合處理給定的輸入標記，從而實現計算資源的動態分配。

**優點**

- **高效預訓練**: 通過利用 MoEs，模型可以用顯著更少的計算資源進行預訓練，從而在與密集模型相同的計算預算內允許更大的模型或數據集規模。
- **更快的推論**: 儘管擁有大量參數，MoEs 只使用一部分進行推論，導致比具有相似參數數量的密集模型更快的處理時間。然而，這種效率伴隨著高記憶體需求的警告，因為需要將所有參數加載到 RAM 中。

**挑戰**

- **訓練泛化**: 雖然 MoEs 在預訓練期間計算效率更高，但它們在微調期間歷來面臨泛化良好的挑戰，經常導致過度擬合。
- **記憶體需求**: MoEs 的高效推論過程需要大量記憶體來載入整個模型的參數，儘管在任何給定的推論任務中只有一小部分是被積極使用的。

**實作細節**

- **參數共享**: 並非所有在 MoE 模型中的參數都是專屬於個別專家的。許多參數是跨模型共享的，這有助於提高效率。例如，在像 Mixtral 8x7B 這樣的 MoE 模型中，由於共享組件，密集等效參數數量可能少於所有專家總和。
- **推論速度**: 推論速度的好處來自於模型僅為每個 token 啟用部分專家，有效地將計算負載減少到一個小得多的模型的水平，同時保持大參數空間的優勢。

### Mamba 模型

Mamba 是一種創新的循環神經網路架構，以其在處理長序列（可能高達 100 萬個元素）方面的效率而著稱。由於其令人印象深刻的延展性和更快的處理能力，這個模型因為成為知名 Transformer 模型的強大競爭者而受到關注。以下是 Mamba 的簡化概述及其重要性：

**Mamba 的核心功能:**

- **線性時間處理**: 不同於 Transformers，其計算和記憶體成本隨著序列長度呈二次方增長，Mamba 在線性時間內運行。這使得它在處理非常長的序列時更加高效。
- **選擇性狀態空間**: Mamba 採用選擇性狀態空間，允許它在任何給定時間專注於數據的相關部分，有效地管理和處理長序列。

Selective State Spaces (SSS) 在像 Mamba 這樣的模型中，指的是一種先進的神經網路架構方法，使模型能夠高效地處理和處理非常長的數據序列。這種方法特別設計用來改進傳統模型如 Transformers 和 Recurrent Neural Networks (RNNs) 在處理長序列時的限制。以下是 Selective State Spaces 背後關鍵概念的分解:

**選擇性狀態空間的基礎:**

- **狀態空間模型 (SSMs)**: 在核心部分，SSS 建立在狀態空間模型的概念之上。SSMs 是一類用於描述隨時間演變的系統的模型，通過響應外部輸入而改變的狀態變量來捕捉動態。SSMs 已經在各個領域中使用，例如信號處理、控制系統，現在也用於 AI 的序列建模。
- **選擇性機制**: "選擇性" 方面引入了一種機制，允許模型在任何給定時間確定輸入序列的哪些部分是相關的。這是通過一個門控或路由函式動態選擇應該根據輸入激活的狀態空間（或模型參數的子集）來實現的。這種選擇性激活有助於模型將其計算資源集中在最相關的數據部分，提高效率。

**傳統模型的優勢：**

- **長序列的效率**: Mamba 的架構經過最佳化以提高速度，提供比 Transformers 快達五倍的吞吐量，同時更有效地處理長序列。
- **多功能性**: 雖然其在基於文本的應用（如聊天機器人和摘要）中表現出色，但 Mamba 在其他需要分析長序列的領域（如音頻生成、基因組學和時間序列數據）中也顯示出潛力。
- **創新設計**: 該模型基於狀態空間模型（S4）建構，但通過引入選擇性結構化狀態空間序列模型，增強了其處理能力。

Mamba 代表了序列建模的一個重大進展，為涉及長序列的任務提供了一個比 Transformers 更高效的替代方案。其能夠隨著序列長度線性延展而不會相應增加計算和記憶體需求，使其成為一個在自然語言處理之外廣泛應用的有前途的工具。

從本質上來說，Mamba 正在重新定義 AI 序列建模的可能性，結合 RNN 和狀態空間模型的優點，並通過創新技術在各個領域實現高效能和高效能。

### **RWKV: 為 Transformer 時代重新發明 RNNs**

RWKV 架構代表了神經網路模型領域中的一種新穎方法，結合了遞迴神經網路 (RNN) 的優勢和 transformer 的變革能力。這種混合架構由 Bo Peng 領導並由活躍的社群支持，旨在解決處理長序列資料的特定挑戰，使其在自然語言處理 (NLP) 和其他應用中顯得特別有趣。

**RWKV 的主要特點:**

- **處理長序列的效率**: 與傳統的 transformer 在序列長度增加時面臨二次方計算和記憶體成本問題不同，RWKV 被設計成可線性擴展。這使得它能夠高效處理顯著長於傳統模型可管理的序列。
- **RNN 和 Transformer 混合體**: RWKV 結合了 RNN 處理序列資料的能力和 transformer 強大的自注意機制。這種融合旨在利用兩者的優點: RNN 的序列資料處理能力和 transformer 的上下文感知和平行處理優勢。
- **創新架構**: RWKV 引入了一種簡化和最佳化的設計，使其能夠有效地作為 RNN 運行。它包含了如 TokenShift 和 SmallInitEmb 等附加功能來提升性能，使其能夠達到與 GPT 模型相當的結果。
- **延展性和性能**: 具備支持訓練高達 14B 參數模型的基礎設施和克服數值不穩定等問題的最佳化，RWKV 提供了一個可擴展且穩健的框架，用於開發先進的 AI 模型。

**相較於傳統模型的優勢:**

- **處理非常長的上下文**: RWKV 可以利用數千個 token 甚至更多的上下文，超越傳統 RNN 的限制，實現更全面的理解和文本生成。
- **平行化訓練**: 與傳統 RNN 難以平行化不同，RWKV 的架構允許更快的訓練，類似於「線性化 GPT」，提供速度和效率。
- **記憶體和速度效率**: RWKV 模型可以在長上下文中訓練和執行，而不需要大型 transformer 的大量 RAM 要求，提供計算資源使用和模型性能之間的平衡。

**應用程式和整合:**

RWKV 的架構使其適用於廣泛的應用，從純語言模型到多模態任務。其整合到 Hugging Face Transformers 函式庫中，方便 AI 社群輕鬆存取和使用，支援各種任務，包括文本生成、聊天機器人等。

總之，RWKV 代表了 AI 研究中的一個令人興奮的發展，它結合了 RNNs 的序列處理優勢與 transformers 的上下文感知和效率。其設計解決了長序列建模中的關鍵挑戰，提供了一個有前途的工具來推進 NLP 和相關領域。

## 閱讀/觀看這些資源 (選擇性)

1. LLM Agents: [https://www.promptingguide.ai/research/llm-agents](https://www.promptingguide.ai/research/llm-agents)
2. LLM Powered Autonomous Agents: [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)
3. Emerging Trends in LLM Architecture- [https://medium.com/@bijit211987/emerging-trends-in-llm-architecture-a8897d9d987b](https://medium.com/@bijit211987/emerging-trends-in-llm-architecture-a8897d9d987b)
4. 四個自ChatGPT以來的LLM趨勢及其對AI建構者的影響: [https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2](https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2)。

## 閱讀這些論文（可選）

1. [https://arxiv.org/abs/2401.13601](https://arxiv.org/abs/2401.13601)
2. [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)
3. [https://arxiv.org/abs/2310.14724](https://arxiv.org/abs/2310.14724)
4. [https://arxiv.org/abs/2307.06435](https://arxiv.org/abs/2307.06435)

